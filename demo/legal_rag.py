#!/usr/bin/env python3
"""
Legal RAG System
Há»‡ thá»‘ng RAG chuyÃªn dá»¥ng cho vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam
Dá»±a trÃªn ConvoRAG nhÆ°ng Ä‘Æ°á»£c tá»‘i Æ°u cho legal domain
"""

import heapq
import numpy as np
import ollama
from typing import List, Dict, Tuple, Any, Optional
import json
import os
from simple_vector_store import SimpleVectorStore, SearchResult
from universal_multi_stage_retrieval import UniversalMultiStageRetrieval
from legal_chunker import FootnoteLookup


class LegalRAG:
    """
    Legal RAG System vá»›i kháº£ nÄƒng:
    - Hiá»ƒu context phÃ¡p luáº­t
    - TrÃ­ch dáº«n chÃ­nh xÃ¡c Ä‘iá»u khoáº£n
    - Xá»­ lÃ½ cÃ¢u há»i phá»©c táº¡p vá» luáº­t
    - Conversation history cho legal consultation
    """
    
    def __init__(self, 
                 vector_store_path: str = "legal_vectors.pkl",
                 llm_model: str = "llama3.2:3b",
                 ollama_host: str = "http://192.168.10.32:11434",
                 debug_mode: bool = False):
        
        self.llm_model = llm_model
        self.ollama_client = ollama.Client(host=ollama_host)
        self.debug_mode = debug_mode
        
        # Load vector store and initialize Universal Multi-stage Retrieval
        self.vector_store = SimpleVectorStore(ollama_host=ollama_host)
        if os.path.exists(vector_store_path):
            self.vector_store.load(vector_store_path)
            print(f"âœ… Loaded legal vector store with {len(self.vector_store.vectors)} chunks")
            
            # Initialize Universal Multi-stage Retrieval as the ONLY retrieval system
            self.retrieval_system = UniversalMultiStageRetrieval(self.vector_store)
            print(f"âœ… Retrieval System: Universal Multi-stage Retrieval")
        else:
            raise FileNotFoundError(f"Vector store not found: {vector_store_path}. Please run simple_vector_store.py first.")
            
        # Conversation history for legal consultation
        self.conversation_history = []
        
        # Initialize footnote lookup
        try:
            with open('../data/01_VBHN-VPQH_363655.txt', 'r', encoding='utf-8') as f:
                self.legal_text = f.read()
            self.footnote_lookup = FootnoteLookup(self.legal_text)
            print(f"âœ… Loaded footnote lookup with {len(self.footnote_lookup.get_all_footnotes())} footnotes")
        except FileNotFoundError:
            print("âš ï¸ Warning: Legal document not found for footnote lookup")
            self.footnote_lookup = None
    
    def debug_print(self, message: str):
        """Print debug message if debug mode is enabled"""
        if self.debug_mode:
            print(message)
    
    def preprocess_legal_query(self, query: str) -> str:
        """Preprocess user query to standardize legal terminology and fix common issues"""
        
        self.debug_print(f"   ğŸ”§ Debug - Original query: '{query}'")
        
        try:
            system_prompt = """
            Báº¡n lÃ  chuyÃªn gia xá»­ lÃ½ ngÃ´n ngá»¯ phÃ¡p luáº­t Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  chuáº©n hÃ³a cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘á»ƒ tÃ¬m kiáº¿m chÃ­nh xÃ¡c trong cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t.

            NHIá»†M Vá»¤:
            1. **Sá»­a lá»—i chÃ­nh táº£** (vÃ­ dá»¥: "ggiay" â†’ "giáº¥y", "lam gia" â†’ "lÃ m giáº£")
            2. **Chuáº©n hÃ³a thuáº­t ngá»¯ phÃ¡p luáº­t**:
               - "lÃ m giáº£ giáº¥y tá»" â†’ "lÃ m giáº£ tÃ i liá»‡u"
               - "chiáº¿m Ä‘oáº¡t" â†’ "cÆ°á»¡ng Ä‘oáº¡t tÃ i sáº£n" 
               - "giáº¿t ngÆ°á»i" â†’ "tá»™i giáº¿t ngÆ°á»i"
               - "trá»™m cáº¯p" â†’ "tá»™i trá»™m cáº¯p tÃ i sáº£n"
            3. **Chuyá»ƒn Ä‘á»•i sá»‘ tiá»n sang Ä‘á»‹nh dáº¡ng chuáº©n**:
               - "700 triá»‡u" â†’ "700.000.000 Ä‘á»“ng"
               - "1 tá»·" â†’ "1.000.000.000 Ä‘á»“ng"
               - "50 ngÃ n" â†’ "50.000 Ä‘á»“ng"
               - "2 trÄƒm triá»‡u" â†’ "200.000.000 Ä‘á»“ng"
            4. **Chuáº©n hÃ³a cáº¥u trÃºc cÃ¢u há»i**:
               - ThÃªm "tá»™i" trÆ°á»›c tÃªn tá»™i pháº¡m náº¿u cáº§n
               - Sá»­ dá»¥ng thuáº­t ngá»¯ chÃ­nh thá»‘ng trong Bá»™ luáº­t HÃ¬nh sá»±

            QUY Táº®C:
            - GIá»® NGUYÃŠN Ã½ nghÄ©a gá»‘c cá»§a cÃ¢u há»i
            - CHá»ˆ chuáº©n hÃ³a thuáº­t ngá»¯ vÃ  sá»­a lá»—i
            - Sá»¬ Dá»¤NG thuáº­t ngá»¯ chÃ­nh xÃ¡c trong Bá»™ luáº­t HÃ¬nh sá»± Viá»‡t Nam
            - TRáº¢ Lá»œI chá»‰ cÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a, khÃ´ng giáº£i thÃ­ch

            VÃ Dá»¤:
            Input: "toi lam gia ggiay to tuy than thi hinh phat la gi"
            Output: "tÃ´i lÃ m giáº£ tÃ i liá»‡u tÃ¹y thÃ¢n thÃ¬ hÃ¬nh pháº¡t lÃ  gÃ¬"

            Input: "chiem doat 700 trieu co bi tu hinh khong"  
            Output: "cÆ°á»¡ng Ä‘oáº¡t tÃ i sáº£n trá»‹ giÃ¡ 700.000.000 Ä‘á»“ng cÃ³ bá»‹ tá»­ hÃ¬nh khÃ´ng"
            """
            
            user_prompt = f'Chuáº©n hÃ³a cÃ¢u há»i: "{query}"'
            
            # Use low temperature for consistent preprocessing
            preprocessed = self.generate_answer(system_prompt, user_prompt, temperature=0.1).strip()
            
            # Clean up response - remove quotes and extra text
            if preprocessed.startswith('"') and preprocessed.endswith('"'):
                preprocessed = preprocessed[1:-1]
            
            # Remove common prefixes from LLM response
            prefixes_to_remove = [
                "CÃ¢u há»i Ä‘Ã£ chuáº©n hÃ³a:",
                "Output:",
                "Káº¿t quáº£:",
                "Chuáº©n hÃ³a:"
            ]
            
            for prefix in prefixes_to_remove:
                if preprocessed.startswith(prefix):
                    preprocessed = preprocessed[len(prefix):].strip()
            
            # Fallback to original if preprocessing seems wrong
            if (len(preprocessed) > len(query) * 2 or 
                len(preprocessed) < len(query) * 0.3 or
                "lá»—i" in preprocessed.lower()):
                self.debug_print(f"   âš ï¸ Preprocessing failed, using original query")
                return query
            
            self.debug_print(f"   âœ… Preprocessed query: '{preprocessed}'")
            return preprocessed
            
        except Exception as e:
            self.debug_print(f"   âŒ Preprocessing error: {str(e)}")
            return query  # Fallback to original query
        
    def enrich_results_with_footnotes(self, search_results: List[SearchResult]) -> List[SearchResult]:
        """Bá»• sung thÃ´ng tin footnote vÃ o search results"""
        if not self.footnote_lookup or not search_results:
            return search_results
            
        enriched_results = []
        for result in search_results:
            # Táº¡o copy cá»§a result Ä‘á»ƒ khÃ´ng modify original
            enriched_result = SearchResult(
                content=result.content,
                score=result.score,
                metadata=result.metadata.copy() if result.metadata else {}
            )
            
            # TÃ¬m footnote trong content
            import re
            footnote_matches = re.findall(r'\[(\d+)\]', result.content)
            
            if footnote_matches:
                footnote_info = []
                for footnote_num in set(footnote_matches):  # Remove duplicates
                    footnote_content = self.footnote_lookup.lookup_footnote(footnote_num)
                    if footnote_content:
                        footnote_info.append(f"[{footnote_num}] {footnote_content}")
                
                if footnote_info:
                    # ThÃªm footnote info vÃ o metadata
                    enriched_result.metadata['footnotes'] = footnote_info
                    
                    # ThÃªm footnote vÃ o cuá»‘i content
                    enriched_result.content += "\n\nğŸ“– CHÃš THÃCH:\n" + "\n".join(footnote_info)
            
            enriched_results.append(enriched_result)
        
        return enriched_results

    def search_legal_context(self, query: str, top_k: int = 10)-> Tuple[str, List[SearchResult]]:
        """Search for relevant legal context using Universal Multi-stage Retrieval"""
        
        # Execute Universal Multi-stage Retrieval
        results, stage_results = self.retrieval_system.execute_multi_stage_retrieval(query, max_results=top_k)
        
        # Enrich results with footnotes
        results = self.enrich_results_with_footnotes(results)
        
        # Format context for LLM with comprehensive error handling
        context_parts = []
        for i, result in enumerate(results):
            try:
                # Debug result structure
                self.debug_print(f"   ğŸ” Debug - Result {i}: type={type(result)}")
                if hasattr(result, '__dict__'):
                    self.debug_print(f"   ğŸ” Debug - Result {i} attrs: {list(result.__dict__.keys())}")
                
                # Safe result access
                if not hasattr(result, 'metadata'):
                    self.debug_print(f"   âš ï¸ Warning - Result {i} has no metadata attribute")
                    article, title, hierarchy = 'N/A', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»', ''
                else:
                    metadata = result.metadata
                    if metadata is None:
                        self.debug_print(f"   âš ï¸ Warning - Result {i} metadata is None")
                        article, title, hierarchy = 'N/A', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»', ''
                    elif not isinstance(metadata, dict):
                        self.debug_print(f"   âš ï¸ Warning - Result {i} metadata not dict: {type(metadata)}")
                        article, title, hierarchy = 'N/A', 'KhÃ´ng cÃ³ tiÃªu Ä‘á»', ''
                    else:
                        # Safe metadata extraction with None checks
                        article = metadata.get('article', 'N/A') if metadata else 'N/A'
                        title = metadata.get('title', '') if metadata else ''
                        hierarchy = metadata.get('hierarchy_path', '') if metadata else ''
                
                # Safe content access
                if not hasattr(result, 'content'):
                    self.debug_print(f"   âš ï¸ Warning - Result {i} has no content attribute")
                    content = 'No content available'
                else:
                    content = result.content if result.content else 'No content available'
                
                # Handle None title
                title_display = title if title else 'KhÃ´ng cÃ³ tiÃªu Ä‘á»'
                
            except Exception as e:
                self.debug_print(f"   âŒ Error processing result {i}: {str(e)}")
                article, title_display, hierarchy, content = 'N/A', 'Lá»—i xá»­ lÃ½', '', 'Error processing result'
            
            formatted_context = f"""
=== {article}. {title_display} ===
[{hierarchy}]

{content}

---
"""
            context_parts.append(formatted_context.strip())
        
        combined_context = '\n\n'.join(context_parts)
        return combined_context, results
    
    
    def detect_query_type(self, query: str) -> str:
        """Classify legal query type using LLM with rule-based fallback"""
        
        # Try LLM-based classification first
        try:
            system_prompt = """
            Báº¡n lÃ  chuyÃªn gia phÃ¢n loáº¡i cÃ¢u há»i phÃ¡p luáº­t. HÃ£y phÃ¢n loáº¡i cÃ¢u há»i vÃ o má»™t trong cÃ¡c loáº¡i:
            
            1. "legal-specific" - CÃ¢u há»i cá»¥ thá»ƒ vá» Ä‘iá»u luáº­t, hÃ¬nh pháº¡t, thá»§ tá»¥c phÃ¡p lÃ½
               VÃ­ dá»¥: "Äiá»u 40 quy Ä‘á»‹nh gÃ¬?", "HÃ¬nh pháº¡t tá»­ hÃ¬nh Ã¡p dá»¥ng nhÆ° tháº¿ nÃ o?"
            
            2. "legal-general" - CÃ¢u há»i chung vá» khÃ¡i niá»‡m phÃ¡p luáº­t
               VÃ­ dá»¥: "TrÃ¡ch nhiá»‡m hÃ¬nh sá»± lÃ  gÃ¬?", "PhÃ¢n loáº¡i tá»™i pháº¡m nhÆ° tháº¿ nÃ o?"
            
            3. "legal-consultation" - CÃ¢u há»i tÆ° váº¥n, Ã¡p dá»¥ng luáº­t vÃ o tÃ¬nh huá»‘ng cá»¥ thá»ƒ  
               VÃ­ dá»¥: "NgÆ°á»i 15 tuá»•i cÃ³ bá»‹ xá»­ lÃ½ hÃ¬nh sá»± khÃ´ng?", "TrÆ°á»ng há»£p nÃ o Ä‘Æ°á»£c miá»…n tÃ¹?"
            
            4. "non-legal" - CÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n phÃ¡p luáº­t
               VÃ­ dá»¥: "Thá»i tiáº¿t hÃ´m nay tháº¿ nÃ o?", "CÃ¡ch náº¥u phá»Ÿ"
            
            Chá»‰ tráº£ lá»i tÃªn loáº¡i, khÃ´ng giáº£i thÃ­ch.
            """
            
            user_prompt = f'PhÃ¢n loáº¡i cÃ¢u há»i: "{query}"'
            
            # Try LLM classification with short timeout
            response = self.generate_answer(system_prompt, user_prompt, temperature=0.1).lower().strip()
            
            # Check if response contains error message
            if "lá»—i" in response.lower():
                raise Exception("LLM classification failed")
            
            if "legal-specific" in response:
                return "legal-specific"
            elif "legal-general" in response:
                return "legal-general" 
            elif "legal-consultation" in response:
                return "legal-consultation"
            elif "non-legal" in response:
                return "non-legal"
            else:
                # If LLM response is unclear, fallback to rule-based
                raise Exception("Unclear LLM response")
                
        except Exception as e:
            self.debug_print(f"   âš ï¸ LLM classification failed ({str(e)[:50]}...), using rule-based fallback")
            return self._rule_based_classification(query)
    
    def _rule_based_classification(self, query: str) -> str:
        """Rule-based fallback for query classification"""
        
        query_lower = query.lower()
        
        # Legal-specific patterns
        specific_patterns = [
            r'Ä‘iá»u\s+\d+', r'khoáº£n\s+\d+', r'chÆ°Æ¡ng\s+[ivx]+',
            'quy Ä‘á»‹nh gÃ¬', 'ná»™i dung', 'Ä‘iá»u khoáº£n', 'luáº­t Ä‘á»‹nh'
        ]
        
        # Legal-general patterns  
        general_patterns = [
            'lÃ  gÃ¬', 'khÃ¡i niá»‡m', 'Ä‘á»‹nh nghÄ©a', 'hiá»ƒu nhÆ° tháº¿ nÃ o',
            'phÃ¢n loáº¡i', 'cÃ¡c loáº¡i', 'nguyÃªn táº¯c'
        ]
        
        # Legal-consultation patterns
        consultation_patterns = [
            'tÃ´i', 'mÃ¬nh', 'cÃ³ bá»‹', 'cÃ³ pháº£i', 'trÆ°á»ng há»£p', 'náº¿u nhÆ°',
            'cÃ³ thá»ƒ', 'Ä‘Æ°á»£c khÃ´ng', 'bá»‹ xá»­ lÃ½', 'pháº¡m tá»™i', 'vi pháº¡m'
        ]
        
        # Non-legal patterns
        non_legal_patterns = [
            'thá»i tiáº¿t', 'Äƒn uá»‘ng', 'du lá»‹ch', 'mua sáº¯m', 'giáº£i trÃ­',
            'thá»ƒ thao', 'Ã¢m nháº¡c', 'phim áº£nh'
        ]
        
        # Check patterns
        import re
        
        # Check non-legal first
        if any(pattern in query_lower for pattern in non_legal_patterns):
            return "non-legal"
        
        # Check specific legal
        if any(re.search(pattern, query_lower) for pattern in specific_patterns):
            return "legal-specific"
        
        # Check general legal
        if any(pattern in query_lower for pattern in general_patterns):
            return "legal-general"
        
        # Check consultation
        if any(pattern in query_lower for pattern in consultation_patterns):
            return "legal-consultation"
        
        # Default for legal queries
        return "legal-consultation"
    
    def generate_answer(self, system_prompt: str, user_prompt: str, 
                       temperature: float = 0.1) -> str:
        """Generate response using LLM with controlled creativity"""
        try:
            response = self.ollama_client.chat(
                model=self.llm_model,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_prompt}
                ],
                options={
                    'temperature': temperature,  # Low for legal accuracy
                    'top_p': 0.3,               # Conservative token selection
                    'top_k': 20,                # Limited vocabulary choices
                    'repeat_penalty': 1.1       # Avoid repetition
                }
            )
            
            # Safe response extraction with comprehensive error checking
            if response is None:
                return "Lá»—i: KhÃ´ng nháº­n Ä‘Æ°á»£c response tá»« LLM"
            
            # Debug: Print response structure
            self.debug_print(f"   ğŸ” Debug - Response type: {type(response)}")
            
            # Handle Ollama ChatResponse object
            if hasattr(response, 'message'):
                # Ollama ChatResponse object
                message = response.message
                if message is None:
                    return "Lá»—i: Response message lÃ  None"
                
                if hasattr(message, 'content'):
                    content = message.content
                    if content is None:
                        return "Lá»—i: Message content lÃ  None"
                    return str(content)
                else:
                    return f"Lá»—i: Message khÃ´ng cÃ³ content attribute. Available attrs: {dir(message)}"
                    
            elif isinstance(response, dict):
                # Dict response format
                self.debug_print(f"   ğŸ” Debug - Response keys: {list(response.keys())}")
                
                if 'message' not in response:
                    return f"Lá»—i: Response thiáº¿u 'message' field. Available keys: {list(response.keys())}"
                
                message = response['message']
                if message is None:
                    return "Lá»—i: Message field lÃ  None"
                
                if not isinstance(message, dict):
                    return f"Lá»—i: Message khÃ´ng pháº£i dict: {type(message)} - {str(message)[:200]}"
                
                if 'content' not in message:
                    return f"Lá»—i: Message thiáº¿u 'content' field. Available keys: {list(message.keys())}"
                
                content = message['content']
                if content is None:
                    return "Lá»—i: Content lÃ  None"
                
                if not isinstance(content, str):
                    return f"Lá»—i: Content khÃ´ng pháº£i string: {type(content)} - {str(content)[:200]}"
                    
                return content
            else:
                return f"Lá»—i: Response format khÃ´ng Ä‘Æ°á»£c há»— trá»£: {type(response)} - {str(response)[:200]}"
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            return f"Lá»—i khi táº¡o cÃ¢u tráº£ lá»i: {str(e)}\nChi tiáº¿t: {error_details}"
    
    def contextualize_query(self, current_query: str) -> str:
        """Enhance query with conversation history"""
        
        if not self.conversation_history:
            return current_query
        
        # If query is very detailed already, don't modify
        if len(current_query.split()) > 15:
            return current_query
        
        # Build conversation context
        history_context = "Lá»‹ch sá»­ trao Ä‘á»•i vá» phÃ¡p luáº­t:\n\n"
        for idx, (q, a) in enumerate(self.conversation_history[-3:]):  # Last 3 exchanges
            history_context += f"CÃ¢u há»i {idx+1}: {q}\nTráº£ lá»i: {a[:200]}...\n\n"
        
        system_prompt = """
        Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  phÃ¢n tÃ­ch cÃ¢u há»i hiá»‡n táº¡i trong bá»‘i cáº£nh 
        cuá»™c trao Ä‘á»•i trÆ°á»›c Ä‘Ã³ vÃ  quyáº¿t Ä‘á»‹nh:
        
        1. CÃ¢u há»i hiá»‡n táº¡i cÃ³ cáº§n thÃªm context tá»« lá»‹ch sá»­ khÃ´ng?
        2. Náº¿u cÃ³, hÃ£y viáº¿t láº¡i cÃ¢u há»i Ä‘á»ƒ nÃ³ Ä‘á»™c láº­p vÃ  Ä‘áº§y Ä‘á»§
        3. Náº¿u khÃ´ng, giá»¯ nguyÃªn cÃ¢u há»i
        
        QUAN TRá»ŒNG:
        - Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong lá»‹ch sá»­ trao Ä‘á»•i
        - KhÃ´ng thÃªm thÃ´ng tin khÃ´ng cÃ³
        - Giá»¯ cÃ¢u há»i ngáº¯n gá»n (dÆ°á»›i 25 tá»«)
        - Chá»‰ tráº£ lá»i cÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c viáº¿t láº¡i, khÃ´ng giáº£i thÃ­ch
        """
        
        user_prompt = f"""
        Lá»‹ch sá»­ trao Ä‘á»•i:
        {history_context}
        
        CÃ¢u há»i hiá»‡n táº¡i: "{current_query}"
        
        CÃ¢u há»i Ä‘Æ°á»£c viáº¿t láº¡i:
        """
        
        reformulated_query = self.generate_answer(system_prompt, user_prompt).strip()
        
        # Clean up response
        if ":" in reformulated_query:
            reformulated_query = reformulated_query.split(":", 1)[1].strip()
        
        # Fallback to original if reformulation seems wrong
        if (len(reformulated_query.split()) > 30 or 
            "dá»±a trÃªn" in reformulated_query.lower() or
            reformulated_query.lower() == current_query.lower()):
            return current_query
        
        return reformulated_query
    
    def handle_non_legal_query(self, query: str) -> str:
        """Handle non-legal queries"""
        
        system_prompt = """
        Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam. NgÆ°á»i dÃ¹ng vá»«a há»i má»™t cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n phÃ¡p luáº­t.
        
        HÃ£y tráº£ lá»i lá»‹ch sá»± vÃ  hÆ°á»›ng dáº«n ngÆ°á»i dÃ¹ng quay láº¡i chá»§ Ä‘á» phÃ¡p luáº­t.
        Giá»¯ cÃ¢u tráº£ lá»i ngáº¯n gá»n (2-3 cÃ¢u).
        """
        
        user_prompt = f'CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: "{query}"'
        
        return self.generate_answer(system_prompt, user_prompt)
    
    def legal_rag(self, query: str) -> str:
        """Main RAG function for legal queries with comprehensive error handling"""
        
        try:
            self.debug_print(f"   ğŸ” Debug - Starting legal_rag with query: '{query[:50]}...'")
            
            # Detect query type
            self.debug_print(f"   ğŸ” Debug - Detecting query type...")
            query_type = self.detect_query_type(query)
            self.debug_print(f"   ğŸ” Debug - Query type: {query_type}")
            
            if query_type == "non-legal":
                response = self.handle_non_legal_query(query)
                self.conversation_history.append((query, response))
                return response
            
            # Contextualize query with conversation history
            self.debug_print(f"   ğŸ” Debug - Contextualizing query...")
            contextualized_query = self.contextualize_query(query)
            self.debug_print(f"   ğŸ” Debug - Contextualized query: '{contextualized_query[:50]}...'")
            
            # Preprocess query to standardize legal terminology
            self.debug_print(f"   ğŸ” Debug - Preprocessing legal query...")
            preprocessed_query = self.preprocess_legal_query(contextualized_query)
            
            # Search for relevant legal context
            self.debug_print(f"   ğŸ” Debug - Searching for legal context...")
            context, search_results = self.search_legal_context(preprocessed_query)
            self.debug_print(f"   ğŸ” Debug - Search completed. Results: {len(search_results) if search_results else 0}")
            
            if not search_results:
                response = "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong Bá»™ luáº­t HÃ¬nh sá»±. Vui lÃ²ng há»i cÃ¢u há»i khÃ¡c hoáº·c cung cáº¥p thÃªm chi tiáº¿t."
                self.conversation_history.append((query, response))
                return response
            
            # Generate answer based on query type with appropriate temperature
            self.debug_print(f"   ğŸ” Debug - Setting up prompts and temperature...")
            if query_type == "legal-specific":
                system_prompt = self._get_specific_legal_prompt()
                temperature = 0.1  # Very conservative for specific legal facts
            elif query_type == "legal-general":
                system_prompt = self._get_general_legal_prompt()
                temperature = 0.2  # Slightly more flexible for explanations
            else:  # legal-consultation
                system_prompt = self._get_consultation_legal_prompt()
                temperature = 0.2  # More flexibility for consultation advice
            
            self.debug_print(f"   ğŸ” Debug - Temperature: {temperature}")
            
            user_prompt = f"""
            Dá»±a trÃªn cÃ¡c Ä‘iá»u khoáº£n phÃ¡p luáº­t sau Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i:

            ÄIá»€U KHOáº¢N LIÃŠN QUAN:
            {context}

            CÃ‚U Há»I Gá»C: {query}
            CÃ‚U Há»I ÄÃƒ CHUáº¨N HÃ“A: {preprocessed_query}

            TRáº¢ Lá»œI:
            """
            
            self.debug_print(f"   ğŸ” Debug - Generating answer...")
            answer = self.generate_answer(system_prompt, user_prompt, temperature)
            self.debug_print(f"   ğŸ” Debug - Answer generated. Length: {len(answer) if answer else 0}")
            
            if not answer or "Lá»—i" in answer:
                return f"âŒ Lá»—i khi táº¡o cÃ¢u tráº£ lá»i: {answer[:200]}..." if answer else "âŒ KhÃ´ng nháº­n Ä‘Æ°á»£c cÃ¢u tráº£ lá»i tá»« há»‡ thá»‘ng"
            
            # Add citation information
            self.debug_print(f"   ğŸ” Debug - Formatting citations...")
            citations = self._format_citations(search_results)
            if citations:
                answer += f"\n\nğŸ“– CÄƒn cá»© phÃ¡p lÃ½:\n{citations}"
            
            # Add confidence and relevance metrics
            confidence_info = self._format_confidence_metrics(search_results, preprocessed_query, query)
            answer += f"\n\n{confidence_info}"
            
            # Update conversation history
            self.conversation_history.append((query, answer))
            
            self.debug_print(f"   âœ… Debug - Legal RAG completed successfully")
            return answer
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.debug_print(f"   âŒ Critical error in legal_rag: {str(e)}")
            self.debug_print(f"   âŒ Full traceback: {error_details}")
            return f"âŒ Lá»—i há»‡ thá»‘ng: {str(e)}\n\nVui lÃ²ng thá»­ láº¡i hoáº·c liÃªn há»‡ há»— trá»£ ká»¹ thuáº­t."
    
    def _get_specific_legal_prompt(self) -> str:
        """System prompt for specific legal queries"""
        return """
        Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t hÃ¬nh sá»± Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i chÃ­nh xÃ¡c 
        cÃ¡c cÃ¢u há»i cá»¥ thá»ƒ vá» Ä‘iá»u luáº­t.

        NGUYÃŠN Táº®C:
        - TrÃ­ch dáº«n chÃ­nh xÃ¡c Ä‘iá»u, khoáº£n, Ä‘iá»ƒm
        - Giáº£i thÃ­ch rÃµ rÃ ng, dá»… hiá»ƒu
        - NÃªu Ä‘áº§y Ä‘á»§ ná»™i dung quy Ä‘á»‹nh
        - KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong Ä‘iá»u khoáº£n

        Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n nhÆ°ng Ä‘áº§y Ä‘á»§ thÃ´ng tin.
        """
    
    def _get_general_legal_prompt(self) -> str:
        """System prompt for general legal queries"""
        return """
        Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t hÃ¬nh sá»± Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  giáº£i thÃ­ch 
        cÃ¡c khÃ¡i niá»‡m vÃ  nguyÃªn táº¯c phÃ¡p luáº­t má»™t cÃ¡ch dá»… hiá»ƒu.

        NGUYÃŠN Táº®C:
        - Giáº£i thÃ­ch khÃ¡i niá»‡m má»™t cÃ¡ch rÃµ rÃ ng
        - ÄÆ°a ra vÃ­ dá»¥ minh há»a khi cáº§n thiáº¿t  
        - LiÃªn káº¿t cÃ¡c Ä‘iá»u khoáº£n liÃªn quan
        - Sá»­ dá»¥ng ngÃ´n ngá»¯ dá»… hiá»ƒu cho ngÆ°á»i dÃ¢n

        Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, cÃ³ cáº¥u trÃºc rÃµ rÃ ng.
        """
    
    def _get_consultation_legal_prompt(self) -> str:
        """System prompt for legal consultation queries"""
        return """
        Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n phÃ¡p luáº­t hÃ¬nh sá»± Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  
        phÃ¢n tÃ­ch tÃ¬nh huá»‘ng vÃ  Ä‘Æ°a ra lá»i khuyÃªn dá»±a trÃªn phÃ¡p luáº­t.

        NGUYÃŠN Táº®C:
        - PhÃ¢n tÃ­ch tÃ¬nh huá»‘ng cá»¥ thá»ƒ
        - Ãp dá»¥ng Ä‘Ãºng Ä‘iá»u khoáº£n phÃ¡p luáº­t
        - ÄÆ°a ra lá»i khuyÃªn thá»±c tiá»…n
        - Cáº£nh bÃ¡o vá» háº­u quáº£ phÃ¡p lÃ½ náº¿u cÃ³
        - Khuyáº¿n nghá»‹ tÃ¬m tÆ° váº¥n chuyÃªn sÃ¢u khi cáº§n

        LÆ¯U Ã: ÄÃ¢y chá»‰ lÃ  tÆ° váº¥n sÆ¡ bá»™, khÃ´ng thay tháº¿ tÆ° váº¥n phÃ¡p lÃ½ chuyÃªn nghiá»‡p.
        
        Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, cÃ³ cáº¥u trÃºc: PhÃ¢n tÃ­ch - Káº¿t luáº­n - Khuyáº¿n nghá»‹.
        """
    
    def _format_citations(self, search_results: List[SearchResult]) -> str:
        """Format legal citations with comprehensive error handling"""
        
        citations = []
        for i, result in enumerate(search_results[:3]):  # Top 3 results
            try:
                # Debug result structure
                self.debug_print(f"   ğŸ” Debug - Citation {i}: type={type(result)}")
                
                # Safe metadata extraction with comprehensive checks
                if not hasattr(result, 'metadata'):
                    self.debug_print(f"   âš ï¸ Warning - Citation {i} has no metadata attribute")
                    article, title, hierarchy = 'N/A', '', ''
                else:
                    metadata = result.metadata
                    if metadata is None:
                        self.debug_print(f"   âš ï¸ Warning - Citation {i} metadata is None")
                        article, title, hierarchy = 'N/A', '', ''
                    elif not isinstance(metadata, dict):
                        self.debug_print(f"   âš ï¸ Warning - Citation {i} metadata not dict: {type(metadata)}")
                        article, title, hierarchy = 'N/A', '', ''
                    else:
                        article = metadata.get('article', 'N/A') if 'article' in metadata else 'N/A'
                        title = metadata.get('title', '') if 'title' in metadata else ''
                        hierarchy = metadata.get('hierarchy_path', '') if 'hierarchy_path' in metadata else ''
                
                # Safe string formatting
                article_str = str(article) if article is not None else 'N/A'
                title_str = str(title) if title is not None else ''
                hierarchy_str = str(hierarchy) if hierarchy is not None else ''
                
                if title_str and title_str.strip():
                    citation = f"â€¢ {article_str}. {title_str} ({hierarchy_str})"
                else:
                    citation = f"â€¢ {article_str} ({hierarchy_str})"
                    
                citations.append(citation)
                
            except Exception as e:
                self.debug_print(f"   âŒ Error formatting citation {i}: {str(e)}")
                citations.append(f"â€¢ Lá»—i xá»­ lÃ½ trÃ­ch dáº«n {i}")
        
        return '\n'.join(citations) if citations else "â€¢ KhÃ´ng cÃ³ trÃ­ch dáº«n"
    
    def _format_confidence_metrics(self, search_results: List[SearchResult], 
                                 preprocessed_query: str, original_query: str) -> str:
        """Format confidence and relevance metrics for answer reliability assessment"""
        
        if not search_results:
            return "ğŸ“Š **Äá»™ tin cáº­y: 0% - KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan**"
        
        try:
            # Calculate metrics
            scores = [result.score for result in search_results[:3]]  # Top 3 results
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            min_score = min(scores)
            
            # Confidence level based on average score
            if avg_score >= 0.8:
                confidence_level = "Ráº¥t cao"
                confidence_color = "ğŸŸ¢"
                confidence_percent = int(avg_score * 100)
            elif avg_score >= 0.7:
                confidence_level = "Cao" 
                confidence_color = "ğŸŸ¡"
                confidence_percent = int(avg_score * 100)
            elif avg_score >= 0.6:
                confidence_level = "Trung bÃ¬nh"
                confidence_color = "ğŸŸ "
                confidence_percent = int(avg_score * 100)
            else:
                confidence_level = "Tháº¥p"
                confidence_color = "ğŸ”´"
                confidence_percent = int(avg_score * 100)
            
            # Query preprocessing effectiveness
            preprocessing_effective = preprocessed_query != original_query
            preprocessing_status = "âœ… ÄÃ£ chuáº©n hÃ³a" if preprocessing_effective else "â– KhÃ´ng cáº§n"
            
            # Results consistency (how close are the top scores)
            if len(scores) > 1:
                score_range = max_score - min_score
                if score_range <= 0.1:
                    consistency = "Cao (káº¿t quáº£ nháº¥t quÃ¡n)"
                elif score_range <= 0.2:
                    consistency = "Trung bÃ¬nh"
                else:
                    consistency = "Tháº¥p (káº¿t quáº£ phÃ¢n tÃ¡n)"
            else:
                consistency = "N/A"
            
            # Format confidence info
            confidence_info = f"""ğŸ“Š **ÄÃ¡nh giÃ¡ Ä‘á»™ tin cáº­y cÃ¢u tráº£ lá»i:**
{confidence_color} **Äá»™ tin cáº­y tá»•ng thá»ƒ:** {confidence_level} ({confidence_percent}%)
ğŸ“ˆ **Äiá»ƒm sá»‘ tÃ¬m kiáº¿m:** Cao nháº¥t: {max_score:.3f} | Trung bÃ¬nh: {avg_score:.3f} | Tháº¥p nháº¥t: {min_score:.3f}
ğŸ”§ **Xá»­ lÃ½ cÃ¢u há»i:** {preprocessing_status}
ğŸ¯ **TÃ­nh nháº¥t quÃ¡n:** {consistency}
ğŸ“‹ **Sá»‘ tÃ i liá»‡u tham kháº£o:** {len(search_results)} Ä‘iá»u khoáº£n

ğŸ’¡ **Khuyáº¿n nghá»‹:** {"CÃ¢u tráº£ lá»i cÃ³ Ä‘á»™ tin cáº­y cao, cÃ³ thá»ƒ tham kháº£o." if avg_score >= 0.7 else "NÃªn tham kháº£o thÃªm Ã½ kiáº¿n chuyÃªn gia hoáº·c tra cá»©u thÃªm tÃ i liá»‡u khÃ¡c."}"""

            return confidence_info
            
        except Exception as e:
            self.debug_print(f"   âŒ Error formatting confidence metrics: {str(e)}")
            return "ğŸ“Š **ÄÃ¡nh giÃ¡ Ä‘á»™ tin cáº­y:** KhÃ´ng thá»ƒ tÃ­nh toÃ¡n (lá»—i há»‡ thá»‘ng)"
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """Get conversation statistics"""
        
        return {
            "total_exchanges": len(self.conversation_history),
            "vector_store_chunks": len(self.vector_store.vectors),
            "llm_model": self.llm_model,
            "embedding_model": self.vector_store.embedding_model,
            "retrieval_system": "Universal Multi-stage Retrieval"
        }


def main():
    """Interactive legal consultation system"""
    
    import sys
    
    # Check for debug mode argument
    debug_mode = '--debug' in sys.argv or '-d' in sys.argv
    
    # Initialize Legal RAG
    print("ğŸ›ï¸  Khá»Ÿi táº¡o Há»‡ thá»‘ng TÆ° váº¥n PhÃ¡p luáº­t HÃ¬nh sá»±...")
    if debug_mode:
        print("ğŸ”§ Debug mode: ENABLED")
    try:
        legal_rag = LegalRAG(debug_mode=debug_mode)
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o há»‡ thá»‘ng: {e}")
        return
    
    print("\n" + "="*60)
    print("ğŸ›ï¸  Há»† THá»NG TÆ¯ Váº¤N PHÃP LUáº¬T HÃŒNH Sá»° VIá»†T NAM")
    print("="*60)
    print("ğŸ“š Dá»±a trÃªn: Bá»™ luáº­t HÃ¬nh sá»± sá»‘ 100/2015/QH13")
    print("ğŸ¤– LLM: Llama3.2:3b | Embedding: nomic-embed-text | Retrieval: Universal Multi-stage")
    print("ğŸ’¡ GÃµ 'exit' Ä‘á»ƒ thoÃ¡t, 'stats' Ä‘á»ƒ xem thá»‘ng kÃª")
    if debug_mode:
        print("ğŸ”§ Debug logs: ON | Sá»­ dá»¥ng 'python legal_rag.py' Ä‘á»ƒ táº¯t debug")
    else:
        print("ğŸ”§ Debug logs: OFF | Sá»­ dá»¥ng 'python legal_rag.py --debug' Ä‘á»ƒ báº­t debug")
    print("="*60)
    
    while True:
        print("\n" + "-"*50)
        user_query = input("â“ CÃ¢u há»i phÃ¡p luáº­t cá»§a báº¡n: ").strip()
        
        if not user_query:
            continue
            
        if user_query.lower() in ['exit', 'quit', 'thoÃ¡t']:
            print("\nğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng há»‡ thá»‘ng tÆ° váº¥n phÃ¡p luáº­t!")
            break
            
        if user_query.lower() in ['stats', 'thá»‘ng kÃª']:
            stats = legal_rag.get_conversation_stats()
            print("\nğŸ“Š THá»NG KÃŠ Há»† THá»NG:")
            for key, value in stats.items():
                print(f"   {key}: {value}")
            continue
        
        
        # Process legal query
        print("\nğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin phÃ¡p luáº­t...")
        try:
            answer = legal_rag.legal_rag(user_query)
            print(f"\nâš–ï¸  Tráº£ lá»i:\n{answer}")
        except Exception as e:
            print(f"\nâŒ Lá»—i: {str(e)}")
        
        print("-"*50)
        continue_query = input("\nâ“ Báº¡n cÃ³ cÃ¢u há»i khÃ¡c khÃ´ng? (y/n): ").strip().lower()
        if continue_query not in ['y', 'yes', 'cÃ³']:
            print("\nğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng há»‡ thá»‘ng tÆ° váº¥n phÃ¡p luáº­t!")
            break


if __name__ == "__main__":
    main()
