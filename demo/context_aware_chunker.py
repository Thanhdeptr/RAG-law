#!/usr/bin/env python3
"""
Context-Aware Legal Document Chunker
Chunker th√¥ng minh b·∫£o to√†n ng·ªØ c·∫£nh khi chia chunk d√†i
"""

import re
import json
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
from pyvi import ViTokenizer

@dataclass
class ContextAwareChunk:
    """Chunk v·ªõi th√¥ng tin ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß"""
    chunk_id: str
    type: str
    content: str
    token_count: int
    title: Optional[str] = None
    
    # Hierarchical information
    part: Optional[str] = None
    part_title: Optional[str] = None
    chapter: Optional[str] = None
    chapter_title: Optional[str] = None
    article: Optional[str] = None
    article_title: Optional[str] = None
    clause: Optional[str] = None
    point: Optional[str] = None
    
    # Context linking
    hierarchy_path: Optional[str] = None
    cross_references: List[str] = None
    footnotes: List[str] = None
    
    # Context preservation
    is_split: bool = False
    parent_chunk_id: Optional[str] = None  # ID c·ªßa chunk g·ªëc tr∆∞·ªõc khi split
    sibling_chunk_ids: List[str] = None    # IDs c·ªßa c√°c chunk anh em
    context_summary: Optional[str] = None  # T√≥m t·∫Øt ng·ªØ c·∫£nh c·ªßa chunk g·ªëc
    split_index: int = 0                   # Th·ª© t·ª± trong c√°c chunk con
    total_splits: int = 1                  # T·ªïng s·ªë chunk con
    
    def __post_init__(self):
        if self.cross_references is None:
            self.cross_references = []
        if self.footnotes is None:
            self.footnotes = []
        if self.sibling_chunk_ids is None:
            self.sibling_chunk_ids = []

class ContextAwareChunker:
    """
    Chunker th√¥ng minh b·∫£o to√†n ng·ªØ c·∫£nh
    """
    
    def __init__(self, max_tokens: int = 180):
        self.max_tokens = max_tokens
        
        # Regex patterns
        self.patterns = {
            'part': re.compile(r'^Ph·∫ßn th·ª© (nh·∫•t|hai|ba|t∆∞|nƒÉm|s√°u|b·∫£y|t√°m|ch√≠n|m∆∞·ªùi)', re.IGNORECASE),
            'chapter': re.compile(r'^Ch∆∞∆°ng ([IVXLCDM]+)$', re.IGNORECASE),
            'article': re.compile(r'^ƒêi·ªÅu (\d+)\.?\s*(.*)$', re.IGNORECASE),
            'clause': re.compile(r'^(\d+)\.(?:\[(\d+)\])?\s*(.*)$'),
            'point': re.compile(r'^([a-z]|ƒë)\)\s*(.*)$'),
            'footnote': re.compile(r'\[(\d+)\]\s*(.*)$'),
            'cross_ref': re.compile(r'(ƒêi·ªÅu \d+|ƒëi·ªÅu \d+|kho·∫£n \d+|ƒëi·ªÉm [a-z])', re.IGNORECASE)
        }
        
        # Context tracking
        self.current_part = None
        self.current_part_title = None
        self.current_chapter = None
        self.current_chapter_title = None
        self.current_article = None
        self.current_article_title = None
        self.part_titles = {}
        self.chapter_titles = {}
        
        # Context mapping for linking
        self.chunk_families = {}  # parent_id -> [child_ids]
    
    def count_tokens(self, text: str) -> int:
        """Count tokens for Vietnamese text"""
        try:
            segmented = ViTokenizer.tokenize(text)
            tokens = segmented.split()
            return len(tokens)
        except Exception as e:
            print(f"Error tokenizing: {e}")
            return len(text.split())
    
    def create_context_summary(self, content: str) -> str:
        """T·∫°o t√≥m t·∫Øt ng·ªØ c·∫£nh cho chunk"""
        # L·∫•y c√¢u ƒë·∫ßu v√† c√¢u cu·ªëi ƒë·ªÉ t·∫°o context summary
        sentences = self._split_by_sentences(content)
        
        if len(sentences) <= 2:
            return content[:200] + "..." if len(content) > 200 else content
        
        # T·∫°o summary t·ª´ c√¢u ƒë·∫ßu, gi·ªØa v√† cu·ªëi
        first_sentence = sentences[0]
        last_sentence = sentences[-1]
        
        # L·∫•y c√¢u gi·ªØa n·∫øu c√≥ nhi·ªÅu c√¢u
        middle_sentence = ""
        if len(sentences) > 4:
            middle_idx = len(sentences) // 2
            middle_sentence = sentences[middle_idx]
        
        summary_parts = [first_sentence]
        if middle_sentence:
            summary_parts.append(middle_sentence)
        summary_parts.append(last_sentence)
        
        return " ... ".join(summary_parts)
    
    def smart_split_with_context(self, content: str, chunk_id: str, metadata: Dict) -> List[ContextAwareChunk]:
        """Chia chunk d√†i nh∆∞ng b·∫£o to√†n ng·ªØ c·∫£nh"""
        chunks = []
        
        # T·∫°o context summary cho chunk g·ªëc
        context_summary = self.create_context_summary(content)
        
        # Chia theo c√¢u
        sentences = self._split_by_sentences(content)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = 0
        
        for sentence in sentences:
            sentence_tokens = self.count_tokens(sentence)
            
            # N·∫øu c√¢u ƒë∆°n l·∫ª qu√° d√†i, chia nh·ªè h∆°n
            if sentence_tokens > self.max_tokens:
                # L∆∞u chunk hi·ªán t·∫°i n·∫øu c√≥
                if current_chunk:
                    chunks.append(self._create_context_chunk(
                        f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                        metadata, chunk_id, context_summary, chunk_index, len(sentences)
                    ))
                    chunk_index += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # Chia c√¢u d√†i th√†nh c√°c ph·∫ßn nh·ªè
                sub_chunks = self._split_long_sentence_with_context(
                    sentence, chunk_id, metadata, context_summary, chunk_index
                )
                chunks.extend(sub_chunks)
                chunk_index += len(sub_chunks)
                continue
            
            # N·∫øu th√™m c√¢u n√†y v∆∞·ª£t qu√° gi·ªõi h·∫°n
            if current_tokens + sentence_tokens > self.max_tokens and current_chunk:
                chunks.append(self._create_context_chunk(
                    f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                    metadata, chunk_id, context_summary, chunk_index, len(sentences)
                ))
                chunk_index += 1
                current_chunk = sentence
                current_tokens = sentence_tokens
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_tokens += sentence_tokens
        
        # Th√™m chunk cu·ªëi
        if current_chunk:
            chunks.append(self._create_context_chunk(
                f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                metadata, chunk_id, context_summary, chunk_index, len(sentences)
            ))
        
        # C·∫≠p nh·∫≠t sibling relationships
        self._update_sibling_relationships(chunks, chunk_id)
        
        return chunks
    
    def _create_context_chunk(self, chunk_id: str, content: str, token_count: int, 
                            metadata: Dict, parent_id: str, context_summary: str, 
                            split_index: int, total_splits: int) -> ContextAwareChunk:
        """T·∫°o chunk v·ªõi th√¥ng tin ng·ªØ c·∫£nh"""
        return ContextAwareChunk(
            chunk_id=chunk_id,
            type='clause',
            content=content,
            token_count=token_count,
            title=metadata.get('title'),
            part=metadata.get('part'),
            part_title=metadata.get('part_title'),
            chapter=metadata.get('chapter'),
            chapter_title=metadata.get('chapter_title'),
            article=metadata.get('article'),
            article_title=metadata.get('article_title'),
            clause=metadata.get('clause'),
            point=metadata.get('point'),
            hierarchy_path=metadata.get('hierarchy_path'),
            cross_references=metadata.get('cross_references', []),
            footnotes=metadata.get('footnotes', []),
            is_split=True,
            parent_chunk_id=parent_id,
            context_summary=context_summary,
            split_index=split_index,
            total_splits=total_splits
        )
    
    def _update_sibling_relationships(self, chunks: List[ContextAwareChunk], parent_id: str):
        """C·∫≠p nh·∫≠t m·ªëi quan h·ªá anh em gi·ªØa c√°c chunks"""
        if len(chunks) <= 1:
            return
        
        # L·∫•y t·∫•t c·∫£ chunk IDs
        sibling_ids = [chunk.chunk_id for chunk in chunks]
        
        # C·∫≠p nh·∫≠t sibling_chunk_ids cho m·ªói chunk
        for chunk in chunks:
            chunk.sibling_chunk_ids = [cid for cid in sibling_ids if cid != chunk.chunk_id]
        
        # L∆∞u v√†o chunk_families
        self.chunk_families[parent_id] = sibling_ids
    
    def _split_long_sentence_with_context(self, sentence: str, chunk_id: str, 
                                        metadata: Dict, context_summary: str, 
                                        start_index: int) -> List[ContextAwareChunk]:
        """Chia c√¢u d√†i v·ªõi ng·ªØ c·∫£nh"""
        chunks = []
        
        # Chia theo d·∫•u ph·∫©y v√† ch·∫•m ph·∫©y
        parts = re.split(r'[,;]\s*', sentence)
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = start_index
        
        for part in parts:
            part_tokens = self.count_tokens(part)
            
            # N·∫øu ph·∫ßn ƒë∆°n l·∫ª v·∫´n qu√° d√†i, chia theo t·ª´
            if part_tokens > self.max_tokens:
                if current_chunk:
                    chunks.append(self._create_context_chunk(
                        f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                        metadata, chunk_id, context_summary, chunk_index, len(parts)
                    ))
                    chunk_index += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # Chia theo t·ª´
                word_chunks = self._split_by_words_with_context(
                    part, chunk_id, metadata, context_summary, chunk_index
                )
                chunks.extend(word_chunks)
                chunk_index += len(word_chunks)
                continue
            
            # N·∫øu th√™m ph·∫ßn n√†y v∆∞·ª£t qu√° gi·ªõi h·∫°n
            if current_tokens + part_tokens > self.max_tokens and current_chunk:
                chunks.append(self._create_context_chunk(
                    f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                    metadata, chunk_id, context_summary, chunk_index, len(parts)
                ))
                chunk_index += 1
                current_chunk = part
                current_tokens = part_tokens
            else:
                current_chunk += ", " + part if current_chunk else part
                current_tokens += part_tokens
        
        # Th√™m chunk cu·ªëi
        if current_chunk:
            chunks.append(self._create_context_chunk(
                f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                metadata, chunk_id, context_summary, chunk_index, len(parts)
            ))
        
        return chunks
    
    def _split_by_words_with_context(self, text: str, chunk_id: str, 
                                   metadata: Dict, context_summary: str, 
                                   start_index: int) -> List[ContextAwareChunk]:
        """Chia theo t·ª´ v·ªõi ng·ªØ c·∫£nh (ph∆∞∆°ng √°n cu·ªëi c√πng)"""
        chunks = []
        words = text.split()
        
        current_chunk = ""
        current_tokens = 0
        chunk_index = start_index
        
        for word in words:
            word_tokens = self.count_tokens(word)
            
            if current_tokens + word_tokens > self.max_tokens and current_chunk:
                chunks.append(self._create_context_chunk(
                    f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                    metadata, chunk_id, context_summary, chunk_index, len(words)
                ))
                chunk_index += 1
                current_chunk = word
                current_tokens = word_tokens
            else:
                current_chunk += " " + word if current_chunk else word
                current_tokens += word_tokens
        
        # Th√™m chunk cu·ªëi
        if current_chunk:
            chunks.append(self._create_context_chunk(
                f"{chunk_id}_part_{chunk_index}", current_chunk, current_tokens, 
                metadata, chunk_id, context_summary, chunk_index, len(words)
            ))
        
        return chunks
    
    def _split_by_sentences(self, text: str) -> List[str]:
        """Chia text theo c√¢u"""
        sentence_endings = r'[.!?]+\s+'
        sentences = re.split(sentence_endings, text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        return sentences
    
    def chunk_document(self, text: str) -> List[ContextAwareChunk]:
        """H√†m chunking ch√≠nh"""
        lines = text.split('\n')
        chunks = []
        
        # Buffer cho clause hi·ªán t·∫°i
        current_clause_content = []
        current_clause_number = None
        current_article_context = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Ki·ªÉm tra header ƒëi·ªÅu
            article_match = self.patterns['article'].match(line)
            if article_match:
                # L∆∞u clause tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_clause_content and current_clause_number:
                    self._process_clause_with_context(current_clause_content, current_clause_number, 
                                                    current_article_context, chunks)
                
                # B·∫Øt ƒë·∫ßu ƒëi·ªÅu m·ªõi
                article_num = article_match.group(1)
                article_title = article_match.group(2).strip()
                
                current_article_context = {
                    'article': f"ƒêi·ªÅu {article_num}",
                    'article_title': article_title,
                    'part': self.current_part,
                    'part_title': self.part_titles.get(self.current_part),
                    'chapter': self.current_chapter,
                    'chapter_title': self.chapter_titles.get(self.current_chapter),
                    'hierarchy_path': f"{self.current_part} > {self.current_chapter}" if self.current_part and self.current_chapter else None
                }
                
                current_clause_content = []
                current_clause_number = None
                continue
            
            # Ki·ªÉm tra kho·∫£n
            clause_match = self.patterns['clause'].match(line)
            if clause_match:
                # L∆∞u clause tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                if current_clause_content and current_clause_number:
                    self._process_clause_with_context(current_clause_content, current_clause_number, 
                                                    current_article_context, chunks)
                
                # B·∫Øt ƒë·∫ßu kho·∫£n m·ªõi
                current_clause_number = clause_match.group(1)
                current_clause_content = [line]
                continue
            
            # Ki·ªÉm tra ph·∫ßn/ch∆∞∆°ng
            part_match = self.patterns['part'].match(line)
            if part_match:
                self.current_part = line
                self.part_titles[line] = None
                continue
            
            chapter_match = self.patterns['chapter'].match(line)
            if chapter_match:
                self.current_chapter = line
                self.chapter_titles[line] = None
                continue
            
            # Ki·ªÉm tra ti√™u ƒë·ªÅ (CH·ªÆ HOA)
            if line.isupper() and len(line) > 5:
                if self.current_part and not self.part_titles.get(self.current_part):
                    self.part_titles[self.current_part] = line
                elif self.current_chapter and not self.chapter_titles.get(self.current_chapter):
                    self.chapter_titles[self.current_chapter] = line
                continue
            
            # Th√™m v√†o clause hi·ªán t·∫°i
            if current_clause_number:
                current_clause_content.append(line)
        
        # X·ª≠ l√Ω clause cu·ªëi
        if current_clause_content and current_clause_number:
            self._process_clause_with_context(current_clause_content, current_clause_number, 
                                            current_article_context, chunks)
        
        return chunks
    
    def _process_clause_with_context(self, clause_content: List[str], clause_number: str, 
                                   article_context: Dict, chunks: List[ContextAwareChunk]):
        """X·ª≠ l√Ω clause v·ªõi b·∫£o to√†n ng·ªØ c·∫£nh"""
        content = '\n'.join(clause_content)
        token_count = self.count_tokens(content)
        
        # T·∫°o metadata
        metadata = {
            'clause': clause_number,
            **article_context
        }
        
        chunk_id = f"{article_context['article'].lower().replace(' ', '_')}_khoan_{clause_number}"
        
        # Ki·ªÉm tra n·∫øu n·ªôi dung v∆∞·ª£t qu√° gi·ªõi h·∫°n token
        if token_count <= self.max_tokens:
            # T·∫°o chunk ƒë∆°n
            chunk = ContextAwareChunk(
                chunk_id=chunk_id,
                type='clause',
                content=content,
                token_count=token_count,
                title=metadata.get('title'),
                part=metadata.get('part'),
                part_title=metadata.get('part_title'),
                chapter=metadata.get('chapter'),
                chapter_title=metadata.get('chapter_title'),
                article=metadata.get('article'),
                article_title=metadata.get('article_title'),
                clause=metadata.get('clause'),
                point=metadata.get('point'),
                hierarchy_path=metadata.get('hierarchy_path'),
                cross_references=metadata.get('cross_references', []),
                footnotes=metadata.get('footnotes', [])
            )
            chunks.append(chunk)
        else:
            # Chia th√†nh nhi·ªÅu chunks v·ªõi ng·ªØ c·∫£nh
            split_chunks = self.smart_split_with_context(content, chunk_id, metadata)
            chunks.extend(split_chunks)
    
    def save_chunks(self, chunks: List[ContextAwareChunk], filename: str):
        """L∆∞u chunks v√†o file JSON"""
        chunks_data = []
        for chunk in chunks:
            chunk_dict = {
                'chunk_id': chunk.chunk_id,
                'type': chunk.type,
                'content': chunk.content,
                'token_count': chunk.token_count,
                'title': chunk.title,
                'part': chunk.part,
                'part_title': chunk.part_title,
                'chapter': chunk.chapter,
                'chapter_title': chunk.chapter_title,
                'article': chunk.article,
                'article_title': chunk.article_title,
                'clause': chunk.clause,
                'point': chunk.point,
                'hierarchy_path': chunk.hierarchy_path,
                'cross_references': chunk.cross_references,
                'footnotes': chunk.footnotes,
                'is_split': chunk.is_split,
                'parent_chunk_id': chunk.parent_chunk_id,
                'sibling_chunk_ids': chunk.sibling_chunk_ids,
                'context_summary': chunk.context_summary,
                'split_index': chunk.split_index,
                'total_splits': chunk.total_splits
            }
            chunks_data.append(chunk_dict)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Saved {len(chunks)} context-aware chunks to {filename}")
    
    def get_chunk_families(self) -> Dict[str, List[str]]:
        """L·∫•y th√¥ng tin c√°c chunk families"""
        return self.chunk_families.copy()

def main():
    """Test context-aware chunker"""
    
    # ƒê·ªçc vƒÉn b·∫£n ph√°p lu·∫≠t
    try:
        with open('../data/01_VBHN-VPQH_363655.txt', 'r', encoding='utf-8') as f:
            text = f.read()
    except FileNotFoundError:
        print("‚ùå Legal document not found. Please ensure the file exists.")
        return
    
    # Kh·ªüi t·∫°o chunker v·ªõi gi·ªõi h·∫°n 180 tokens
    chunker = ContextAwareChunker(max_tokens=180)
    
    print("üîß Creating context-aware chunks...")
    chunks = chunker.chunk_document(text)
    
    print(f"üìä Generated {len(chunks)} chunks")
    
    # Ph√¢n t√≠ch k·∫øt qu·∫£
    token_counts = [chunk.token_count for chunk in chunks]
    max_tokens = max(token_counts)
    avg_tokens = sum(token_counts) / len(token_counts)
    over_limit = sum(1 for tc in token_counts if tc > 180)
    split_chunks = [c for c in chunks if c.is_split]
    
    print(f"üìà Statistics:")
    print(f"   Average tokens: {avg_tokens:.1f}")
    print(f"   Max tokens: {max_tokens}")
    print(f"   Chunks over 180 tokens: {over_limit}")
    print(f"   Split chunks: {len(split_chunks)}")
    print(f"   Chunk families: {len(chunker.get_chunk_families())}")
    
    # Hi·ªÉn th·ªã v√≠ d·ª• v·ªÅ context linking
    if split_chunks:
        print(f"\nüîó Context Linking Examples:")
        for chunk in split_chunks[:3]:
            print(f"   {chunk.chunk_id}:")
            print(f"     Parent: {chunk.parent_chunk_id}")
            print(f"     Siblings: {chunk.sibling_chunk_ids}")
            print(f"     Context: {chunk.context_summary[:100]}...")
            print()
    
    # L∆∞u chunks
    chunker.save_chunks(chunks, "context_aware_chunks.json")
    
    print(f"\n‚úÖ Context-aware chunking complete!")

if __name__ == "__main__":
    main()

