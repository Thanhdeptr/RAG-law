#!/usr/bin/env python3
"""
Context-Aware Retrieval System with HuggingFace Embedding
Há»‡ thá»‘ng retrieval thÃ´ng minh vá»›i model huyydangg/DEk21_hcmute_embedding
"""

import json
from typing import List, Dict, Any, Set, Tuple
from dataclasses import dataclass
from context_aware_vector_store import ContextAwareVectorStore, SearchResult

@dataclass
class ContextAwareSearchResult:
    """Káº¿t quáº£ tÃ¬m kiáº¿m vá»›i ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§"""
    primary_chunk: SearchResult
    related_chunks: List[SearchResult]
    context_summary: str
    total_chunks: int
    confidence_score: float

class ContextAwareRetrievalHF:
    """
    Há»‡ thá»‘ng retrieval thÃ´ng minh vá»›i HuggingFace embedding
    """
    
    def __init__(self, vector_store: ContextAwareVectorStore, chunks_file: str = "context_aware_chunks.json"):
        self.vector_store = vector_store
        self.chunks_metadata = self._load_chunks_metadata(chunks_file)
        self.chunk_families = self._build_chunk_families()
    
    def _load_chunks_metadata(self, chunks_file: str) -> Dict[str, Dict]:
        """Load metadata cá»§a chunks"""
        try:
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            # Táº¡o mapping tá»« chunk_id Ä‘áº¿n metadata
            metadata_map = {}
            for chunk in chunks:
                metadata_map[chunk['chunk_id']] = chunk
            
            print(f"âœ… Loaded metadata for {len(metadata_map)} chunks")
            return metadata_map
            
        except FileNotFoundError:
            print(f"âš ï¸ {chunks_file} not found. Context linking disabled.")
            return {}
    
    def _build_chunk_families(self) -> Dict[str, List[str]]:
        """XÃ¢y dá»±ng mapping cÃ¡c chunk families"""
        families = {}
        
        for chunk_id, metadata in self.chunks_metadata.items():
            if metadata.get('is_split', False):
                parent_id = metadata.get('parent_chunk_id')
                if parent_id:
                    if parent_id not in families:
                        families[parent_id] = []
                    families[parent_id].append(chunk_id)
        
        print(f"âœ… Built {len(families)} chunk families")
        return families
    
    def search_with_context(self, query: str, top_k: int = 5, 
                          include_siblings: bool = True,
                          include_parent: bool = True) -> List[ContextAwareSearchResult]:
        """
        TÃ¬m kiáº¿m vá»›i ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§
        
        Args:
            query: CÃ¢u há»i tÃ¬m kiáº¿m
            top_k: Sá»‘ lÆ°á»£ng káº¿t quáº£ chÃ­nh
            include_siblings: CÃ³ bao gá»“m chunks anh em khÃ´ng
            include_parent: CÃ³ bao gá»“m chunk cha khÃ´ng
        """
        
        # TÃ¬m kiáº¿m cÆ¡ báº£n
        primary_results = self.vector_store.search(query, top_k=top_k)
        
        context_results = []
        
        for primary_result in primary_results:
            # Láº¥y chunks liÃªn quan
            related_chunks = self._get_related_chunks(
                primary_result.chunk_id, include_siblings, include_parent
            )
            
            # Táº¡o context summary
            context_summary = self._create_context_summary(primary_result, related_chunks)
            
            # TÃ­nh confidence score
            confidence_score = self._calculate_confidence_score(primary_result, related_chunks)
            
            # Táº¡o káº¿t quáº£ vá»›i ngá»¯ cáº£nh
            context_result = ContextAwareSearchResult(
                primary_chunk=primary_result,
                related_chunks=related_chunks,
                context_summary=context_summary,
                total_chunks=len(related_chunks) + 1,
                confidence_score=confidence_score
            )
            
            context_results.append(context_result)
        
        return context_results
    
    def _get_related_chunks(self, chunk_id: str, include_siblings: bool, 
                          include_parent: bool) -> List[SearchResult]:
        """Láº¥y cÃ¡c chunks liÃªn quan"""
        related_chunks = []
        
        if chunk_id not in self.chunks_metadata:
            return related_chunks
        
        metadata = self.chunks_metadata[chunk_id]
        
        # Láº¥y sibling chunks
        if include_siblings and metadata.get('is_split', False):
            sibling_ids = metadata.get('sibling_chunk_ids', [])
            for sibling_id in sibling_ids:
                sibling_chunk = self.vector_store.get_chunk(sibling_id)
                if sibling_chunk:
                    related_chunks.append(sibling_chunk)
        
        # Láº¥y parent chunk
        if include_parent and metadata.get('is_split', False):
            parent_id = metadata.get('parent_chunk_id')
            if parent_id:
                parent_chunk = self.vector_store.get_chunk(parent_id)
                if parent_chunk:
                    related_chunks.append(parent_chunk)
        
        return related_chunks
    
    def _create_context_summary(self, primary_chunk: SearchResult, 
                              related_chunks: List[SearchResult]) -> str:
        """Táº¡o tÃ³m táº¯t ngá»¯ cáº£nh"""
        
        # Láº¥y context summary tá»« metadata náº¿u cÃ³
        if primary_chunk.chunk_id in self.chunks_metadata:
            metadata = self.chunks_metadata[primary_chunk.chunk_id]
            context_summary = metadata.get('context_summary')
            if context_summary:
                return context_summary
        
        # Náº¿u khÃ´ng cÃ³ context summary, táº¡o tá»« ná»™i dung
        all_content = [primary_chunk.content]
        for chunk in related_chunks:
            all_content.append(chunk.content)
        
        # Táº¡o summary tá»« ná»™i dung
        combined_content = " ".join(all_content)
        
        # Láº¥y cÃ¢u Ä‘áº§u vÃ  cuá»‘i
        sentences = combined_content.split('.')
        if len(sentences) <= 2:
            return combined_content[:300] + "..." if len(combined_content) > 300 else combined_content
        
        first_sentence = sentences[0]
        last_sentence = sentences[-1]
        
        return f"{first_sentence}... {last_sentence}"
    
    def _calculate_confidence_score(self, primary_chunk: SearchResult, 
                                  related_chunks: List[SearchResult]) -> float:
        """TÃ­nh Ä‘iá»ƒm tin cáº­y dá»±a trÃªn ngá»¯ cáº£nh"""
        
        # Äiá»ƒm cÆ¡ báº£n tá»« primary chunk
        base_score = primary_chunk.score
        
        # Bonus cho viá»‡c cÃ³ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§
        context_bonus = 0.0
        if related_chunks:
            # CÃ ng nhiá»u chunks liÃªn quan, Ä‘iá»ƒm cÃ ng cao
            context_bonus = min(0.2, len(related_chunks) * 0.05)
        
        # Bonus cho chunks khÃ´ng bá»‹ split (ngá»¯ cáº£nh nguyÃªn váº¹n)
        integrity_bonus = 0.0
        if primary_chunk.chunk_id in self.chunks_metadata:
            metadata = self.chunks_metadata[primary_chunk.chunk_id]
            if not metadata.get('is_split', False):
                integrity_bonus = 0.1
        
        final_score = min(1.0, base_score + context_bonus + integrity_bonus)
        return final_score
    
    def get_full_context(self, chunk_id: str) -> Dict[str, Any]:
        """Láº¥y ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ cá»§a má»™t chunk"""
        
        if chunk_id not in self.chunks_metadata:
            return {"error": "Chunk not found"}
        
        metadata = self.chunks_metadata[chunk_id]
        
        context = {
            "chunk_id": chunk_id,
            "content": metadata.get('content', ''),
            "token_count": metadata.get('token_count', 0),
            "article": metadata.get('article', ''),
            "clause": metadata.get('clause', ''),
            "hierarchy_path": metadata.get('hierarchy_path', ''),
            "is_split": metadata.get('is_split', False),
            "context_summary": metadata.get('context_summary', ''),
            "related_chunks": []
        }
        
        # ThÃªm thÃ´ng tin chunks liÃªn quan
        if metadata.get('is_split', False):
            context["parent_chunk_id"] = metadata.get('parent_chunk_id')
            context["sibling_chunk_ids"] = metadata.get('sibling_chunk_ids', [])
            context["split_index"] = metadata.get('split_index', 0)
            context["total_splits"] = metadata.get('total_splits', 1)
            
            # Láº¥y ná»™i dung cá»§a sibling chunks
            for sibling_id in metadata.get('sibling_chunk_ids', []):
                if sibling_id in self.chunks_metadata:
                    sibling_metadata = self.chunks_metadata[sibling_id]
                    context["related_chunks"].append({
                        "chunk_id": sibling_id,
                        "content": sibling_metadata.get('content', ''),
                        "split_index": sibling_metadata.get('split_index', 0)
                    })
        
        return context
    
    def format_context_response(self, context_results: List[ContextAwareSearchResult]) -> str:
        """Format káº¿t quáº£ tÃ¬m kiáº¿m vá»›i ngá»¯ cáº£nh"""
        
        if not context_results:
            return "KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p."
        
        response_parts = []
        
        for i, result in enumerate(context_results, 1):
            # ThÃ´ng tin chunk chÃ­nh
            primary = result.primary_chunk
            article = primary.metadata.get('article', 'N/A') if primary.metadata else 'N/A'
            clause = primary.metadata.get('clause', 'N/A') if primary.metadata else 'N/A'
            
            response_parts.append(f"=== Káº¾T QUáº¢ {i} ===")
            response_parts.append(f"ğŸ“‹ {article} - Khoáº£n {clause}")
            response_parts.append(f"ğŸ¯ Äá»™ tin cáº­y: {result.confidence_score:.2f}")
            response_parts.append(f"ğŸ“Š Tá»•ng chunks: {result.total_chunks}")
            
            # Context summary
            if result.context_summary:
                response_parts.append(f"ğŸ“– Ngá»¯ cáº£nh: {result.context_summary}")
            
            # Ná»™i dung chunk chÃ­nh
            response_parts.append(f"ğŸ“ Ná»™i dung chÃ­nh:")
            response_parts.append(primary.content)
            
            # Chunks liÃªn quan
            if result.related_chunks:
                response_parts.append(f"\nğŸ”— Chunks liÃªn quan ({len(result.related_chunks)} chunks):")
                for j, related in enumerate(result.related_chunks, 1):
                    related_article = related.metadata.get('article', 'N/A') if related.metadata else 'N/A'
                    related_clause = related.metadata.get('clause', 'N/A') if related.metadata else 'N/A'
                    response_parts.append(f"   {j}. {related_article} - Khoáº£n {related_clause}")
                    response_parts.append(f"      {related.content[:200]}...")
            
            response_parts.append("\n" + "-" * 60 + "\n")
        
        return "\n".join(response_parts)

def main():
    """Test context-aware retrieval with HuggingFace embedding"""
    
    print("ğŸ” Testing Context-Aware Retrieval with HuggingFace Embedding")
    print("=" * 70)
    
    # Load vector store
    vector_store = ContextAwareVectorStore()
    if not vector_store.load('context_aware_vectors.pkl'):
        print("âŒ Cannot load vector store. Please run context_aware_vector_store.py first.")
        return
    
    # Initialize context-aware retrieval
    retrieval = ContextAwareRetrievalHF(vector_store, "context_aware_chunks.json")
    
    # Test queries
    test_queries = [
        "NgÆ°á»i lao Ä‘á»™ng cÃ³ quyá»n nghá»‰ thai sáº£n bao lÃ¢u?",
        "HÃ¬nh pháº¡t tá»­ hÃ¬nh Ã¡p dá»¥ng nhÆ° tháº¿ nÃ o?",
        "Äiá»u kiá»‡n miá»…n trÃ¡ch nhiá»‡m hÃ¬nh sá»±",
        "TrÃ¡ch nhiá»‡m hÃ¬nh sá»± cá»§a ngÆ°á»i dÆ°á»›i 16 tuá»•i",
        "HÃ¬nh pháº¡t cho tá»™i trá»™m cáº¯p tÃ i sáº£n"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Query: {query}")
        print("-" * 50)
        
        # Search with context
        results = retrieval.search_with_context(query, top_k=2)
        
        # Format and display results
        formatted_response = retrieval.format_context_response(results)
        print(formatted_response)
        
        # Show detailed context information
        if results:
            print("ğŸ” DETAILED CONTEXT ANALYSIS:")
            for i, result in enumerate(results, 1):
                print(f"\nResult {i}:")
                print(f"  Primary chunk: {result.primary_chunk.chunk_id}")
                print(f"  Score: {result.primary_chunk.score:.3f}")
                print(f"  Token count: {result.primary_chunk.metadata.get('token_count', 'N/A')}")
                print(f"  Is split: {result.primary_chunk.metadata.get('is_split', False)}")
                if result.primary_chunk.metadata.get('is_split'):
                    print(f"  Parent: {result.primary_chunk.metadata.get('parent_chunk_id', 'N/A')}")
                    print(f"  Siblings: {result.primary_chunk.metadata.get('sibling_chunk_ids', [])}")
                print(f"  Related chunks: {len(result.related_chunks)}")
                print(f"  Confidence: {result.confidence_score:.3f}")
        
        input("\nPress Enter to continue to next query...")

if __name__ == "__main__":
    main()

