#!/usr/bin/env python3
"""
Universal Multi-stage Retrieval for Legal RAG
Tá»•ng quÃ¡t hÃ³a cho táº¥t cáº£ chá»§ Ä‘á» phÃ¡p luáº­t, khÃ´ng hardcode specific domains
"""

import re
import numpy as np
from typing import List, Dict, Tuple, Any, Optional, Set
from dataclasses import dataclass
from simple_vector_store import SimpleVectorStore, SearchResult


@dataclass
class RetrievalStageResult:
    """Káº¿t quáº£ tá»« má»™t stage cá»§a retrieval"""
    stage_name: str
    results: List[SearchResult]
    query_variants: List[str]
    execution_time: float


class UniversalMultiStageRetrieval:
    """
    Universal Multi-stage Retrieval System
    KhÃ´ng hardcode chá»§ Ä‘á» cá»¥ thá»ƒ, tá»± Ä‘á»™ng detect vÃ  adapt
    """
    
    def __init__(self, vector_store: SimpleVectorStore, enable_query_enhancement: bool = True):
        self.vector_store = vector_store
        self.stage_results = []
        self.enable_query_enhancement = enable_query_enhancement
        
        # Universal legal patterns (khÃ´ng specific vá» chá»§ Ä‘á»)
        self.legal_patterns = self._build_universal_patterns()
        self.topic_keywords = self._build_topic_keywords()
        
        # Stage weights for consensus scoring
        self.stage_weights = {
            'stage1_semantic': 1.0,      # Semantic similarity - baseline
            'stage3_keyword': 0.8,       # Keyword matching - slightly lower
            'stage5_crossref': 0.9,      # Cross-reference - important
            'stage6_exact': 2.0          # Exact metadata - highest priority
        }
        
        # Initialize LLM Query Enhancer if enabled
        self.query_enhancer = None
        if enable_query_enhancement:
            try:
                from llm_query_enhancer import LLMQueryEnhancer
                self.query_enhancer = LLMQueryEnhancer()
                print("âœ… LLM Query Enhancer initialized")
            except Exception as e:
                print(f"âš ï¸ LLM Query Enhancer initialization failed: {e}")
                self.enable_query_enhancement = False
        
    def _build_universal_patterns(self) -> Dict[str, List[str]]:
        """XÃ¢y dá»±ng patterns tá»•ng quÃ¡t cho phÃ¡p luáº­t - tá»‘i Æ°u cho clause-level chunks"""
        
        return {
            "legal_prefixes": [
                "Bá»™ luáº­t HÃ¬nh sá»± quy Ä‘á»‹nh",
                "phÃ¡p luáº­t vá»",
                "Ä‘iá»u khoáº£n",
                "quy Ä‘á»‹nh phÃ¡p lÃ½",
                "theo luáº­t Ä‘á»‹nh",
                "khoáº£n",
                "Ä‘iá»u",
                "Ä‘iá»ƒm"
            ],
            
            "question_types": {
                "definition": ["lÃ  gÃ¬", "khÃ¡i niá»‡m", "Ä‘á»‹nh nghÄ©a", "hiá»ƒu nhÆ° tháº¿ nÃ o"],
                "procedure": ["thá»§ tá»¥c", "quy trÃ¬nh", "cÃ¡ch thá»©c", "lÃ m sao"],
                "penalty": ["hÃ¬nh pháº¡t", "xá»­ lÃ½", "pháº¡t", "tÃ¹", "tiá»n"],
                "clause_specific": ["khoáº£n", "Ä‘iá»u", "Ä‘iá»ƒm", "má»¥c"],
                "condition": ["Ä‘iá»u kiá»‡n", "yÃªu cáº§u", "khi nÃ o", "trÆ°á»ng há»£p"],
                "comparison": ["khÃ¡c nhau", "giá»‘ng", "so sÃ¡nh", "phÃ¢n biá»‡t"],
                "exception": ["ngoáº¡i lá»‡", "trá»«", "khÃ´ng Ã¡p dá»¥ng", "miá»…n"]
            },
            
            "legal_verbs": [
                "vi pháº¡m", "xÃ¢m pháº¡m", "thá»±c hiá»‡n", "gÃ¢y ra", "cÃ³ hÃ nh vi",
                "pháº¡m tá»™i", "lÃ m trÃ¡i", "khÃ´ng tuÃ¢n thá»§", "vi pháº¡m",
                "xá»­ lÃ½", "xÃ©t xá»­", "truy cá»©u", "káº¿t Ã¡n", "tuyÃªn pháº¡t",
                "miá»…n", "giáº£m", "tÄƒng", "Ã¡p dá»¥ng", "ban hÃ nh"
            ],
            
            "legal_nouns": [
                "tá»™i pháº¡m", "vi pháº¡m", "hÃ nh vi", "háº­u quáº£", "thiá»‡t háº¡i",
                "chá»©ng cá»©", "báº±ng chá»©ng", "cÄƒn cá»©", "cÆ¡ sá»Ÿ", "lÃ½ do",
                "quyá»n", "nghÄ©a vá»¥", "trÃ¡ch nhiá»‡m", "quyá»n lá»£i", "lá»£i Ã­ch"
            ]
        }
    
    def _build_topic_keywords(self) -> Dict[str, List[str]]:
        """XÃ¢y dá»±ng tá»« khÃ³a cho cÃ¡c chá»§ Ä‘á» phÃ¡p luáº­t (tá»± Ä‘á»™ng detect)"""
        
        return {
            "general_responsibility": ["trÃ¡ch nhiá»‡m", "xá»­ lÃ½", "tuá»•i", "nÄƒng lá»±c"],
            
            "crimes_against_person": [
                "giáº¿t ngÆ°á»i", "cá»‘ Ã½ gÃ¢y thÆ°Æ¡ng tÃ­ch", "hiáº¿p dÃ¢m", "cÆ°á»¡ng dÃ¢m",
                "báº¯t cÃ³c", "giam giá»¯", "Ä‘e dá»a", "xÃºc pháº¡m", "danh dá»±"
            ],
            
            "crimes_against_property": [
                "trá»™m cáº¯p", "cÆ°á»›p giáº­t", "cÆ°á»›p tÃ i sáº£n", "lá»«a Ä‘áº£o",
                "chiáº¿m Ä‘oáº¡t", "tÃ i sáº£n", "cá»§a cáº£i", "tiá»n báº¡c"
            ],
            
            "drug_crimes": [
                "ma tÃºy", "cháº¥t kÃ­ch thÃ­ch", "tÃ ng trá»¯", "mua bÃ¡n",
                "váº­n chuyá»ƒn", "sá»­ dá»¥ng trÃ¡i phÃ©p", "cháº¥t gÃ¢y nghiá»‡n"
            ],
            
            "corruption": [
                "tham nhÅ©ng", "nháº­n há»‘i lá»™", "Ä‘Æ°a há»‘i lá»™", "láº¡m dá»¥ng chá»©c vá»¥",
                "tham Ã´", "chiáº¿m Ä‘oáº¡t", "chá»©c vá»¥", "quyá»n háº¡n"
            ],
            
            "environmental_crimes": [
                "mÃ´i trÆ°á»ng", "Ã´ nhiá»…m", "phÃ¡ rá»«ng", "khai thÃ¡c",
                "cháº¥t tháº£i", "tÃ i nguyÃªn", "sinh thÃ¡i", "báº£o vá»‡"
            ],
            
            "cybercrime": [
                "mÃ¡y tÃ­nh", "máº¡ng", "thÃ´ng tin", "dá»¯ liá»‡u", "hack",
                "truy cáº­p trÃ¡i phÃ©p", "cÃ´ng nghá»‡", "internet", "website"
            ],
            
            "economic_crimes": [
                "kinh táº¿", "thuáº¿", "tÃ i chÃ­nh", "ngÃ¢n hÃ ng", "Ä‘áº§u tÆ°",
                "chá»©ng khoÃ¡n", "tiá»n tá»‡", "giao dá»‹ch", "thÆ°Æ¡ng máº¡i"
            ],
            
            "traffic_crimes": [
                "giao thÃ´ng", "lÃ¡i xe", "tai náº¡n", "rÆ°á»£u bia", "tá»‘c Ä‘á»™",
                "vi pháº¡m luáº­t", "phÆ°Æ¡ng tiá»‡n", "Ä‘Æ°á»ng bá»™"
            ],
            
            "administrative_crimes": [
                "hÃ nh chÃ­nh", "cÃ´ng vá»¥", "cÃ¡n bá»™", "viÃªn chá»©c", "cÃ´ng chá»©c",
                "thá»§ tá»¥c", "giáº¥y tá»", "chá»©ng thá»±c", "cÃ´ng quyá»n"
            ]
        }
    
    
    def extract_universal_keywords(self, query: str) -> Dict[str, List[str]]:
        """Extract keywords tá»•ng quÃ¡t khÃ´ng bias chá»§ Ä‘á»"""
        
        keywords = {
            "legal_verbs": [],
            "legal_nouns": [],
            "numbers": [],
            "entities": [],
            "core_terms": []
        }
        
        query_lower = query.lower()
        
        # Extract legal verbs
        for verb in self.legal_patterns["legal_verbs"]:
            if verb in query_lower:
                keywords["legal_verbs"].append(verb)
        
        # Extract legal nouns  
        for noun in self.legal_patterns["legal_nouns"]:
            if noun in query_lower:
                keywords["legal_nouns"].append(noun)
        
        # Extract numbers
        numbers = re.findall(r'\d+', query)
        keywords["numbers"] = numbers
        
        # Extract potential entities (proper nouns, specific terms)
        # Simple approach: words that start with capital letters
        entities = re.findall(r'\b[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+\b', query)
        keywords["entities"] = entities
        
        # Extract core terms (split query into words, remove stop words)
        stop_words = {"cá»§a", "vÃ ", "vá»›i", "vá»", "trong", "tá»«", "Ä‘áº¿n", "cÃ³", "lÃ ", "Ä‘Æ°á»£c", "cho", "khi", "nÃ o", "nhÆ°", "tháº¿", "sao", "gÃ¬", "bao", "nhiá»u"}
        words = query_lower.split()
        core_terms = [word for word in words if len(word) > 2 and word not in stop_words]
        keywords["core_terms"] = core_terms
        
        return keywords
    
    def stage1_universal_semantic(self, query: str) -> RetrievalStageResult:
        """Stage 1: Universal semantic search - khÃ´ng bias chá»§ Ä‘á»"""
        
        import time
        start_time = time.time()
        
        print(f"ğŸ” Stage 1: Universal Semantic Search")
        print(f"   Query: '{query}'")
        
        # Direct semantic search
        results = self.vector_store.search(query, top_k=7)
        
        print(f"   â†’ Found {len(results)} results")
        for i, result in enumerate(results[:3], 1):
            article = result.metadata.get('article', 'N/A') if result.metadata else 'N/A'
            title = result.metadata.get('title', 'N/A') if result.metadata else 'N/A'
            # Safe string slicing - handle None values
            title_safe = str(title) if title is not None else 'N/A'
            title_short = title_safe[:40] if len(title_safe) > 40 else title_safe
            print(f"   {i}. {article} - {title_short}... (score: {result.score:.3f})")
        
        execution_time = time.time() - start_time
        
        return RetrievalStageResult(
            stage_name="Universal Semantic",
            results=results,
            query_variants=[query],
            execution_time=execution_time
        )
    
    
    def stage3_keyword_extraction(self, query: str) -> RetrievalStageResult:
        """Stage 3: Universal keyword extraction vÃ  search"""
        
        import time
        start_time = time.time()
        
        print(f"ğŸ”‘ Stage 3: Universal Keyword Extraction")
        
        all_results = []
        query_variants = []
        
        # Extract keywords
        keywords_dict = self.extract_universal_keywords(query)
        
        print(f"   Extracted keywords:")
        for k_type, k_list in keywords_dict.items():
            if k_list:
                print(f"   â€¢ {k_type}: {k_list}")
        
        # Search vá»›i tá»«ng loáº¡i keyword
        for keyword_type, keyword_list in keywords_dict.items():
            for keyword in keyword_list:
                if len(keyword) > 2:  # Skip very short keywords
                    results = self.vector_store.search(keyword, top_k=4)
                    all_results.extend(results)
                    query_variants.append(keyword)
                    
                    if results and results[0].score > 0.5:  # Only log good results
                        print(f"   â€¢ '{keyword}' â†’ {len(results)} results (best: {results[0].score:.3f})")
        
        execution_time = time.time() - start_time
        
        return RetrievalStageResult(
            stage_name="Universal Keywords",
            results=all_results,
            query_variants=query_variants,
            execution_time=execution_time
        )
    
    
    def stage5_cross_reference_search(self, query: str) -> RetrievalStageResult:
        """Stage 5: Cross-reference vÃ  hierarchical search"""
        
        import time
        start_time = time.time()
        
        print(f"ğŸ”— Stage 5: Cross-reference Search")
        
        all_results = []
        query_variants = []
        
        # A. TÃ¬m cross-references trong query
        cross_refs = self.find_cross_references(query)
        
        if cross_refs:
            print(f"   Found cross-references: {cross_refs}")
            
            for ref in cross_refs:
                results = self.vector_store.search(ref, top_k=4)
                all_results.extend(results)
                query_variants.append(f"cross_ref:{ref}")
        
        # B. Hierarchical search (search trong cÃ¹ng chapter/part)
        # Náº¿u Ä‘Ã£ cÃ³ results tá»« previous stages, tÃ¬m trong cÃ¹ng hierarchy
        if hasattr(self, 'previous_results') and self.previous_results:
            hierarchies = self.extract_hierarchies_from_results(self.previous_results)
            
            for hierarchy in hierarchies:
                # Search trong cÃ¹ng chapter/part
                hierarchy_results = self.vector_store.search_by_metadata(hierarchy, top_k=5)
                
                # Filter by semantic relevance
                for result in hierarchy_results:
                    if self.is_semantically_relevant(query, result.content):
                        all_results.append(result)
                        query_variants.append(f"hierarchy:{hierarchy}")
        
        execution_time = time.time() - start_time
        
        return RetrievalStageResult(
            stage_name="Cross-reference",
            results=all_results,
            query_variants=query_variants,
            execution_time=execution_time
        )
    
    def stage6_exact_metadata_search(self, query: str) -> RetrievalStageResult:
        """Stage 6: Exact metadata matching - match vá»›i metadata cá»§a chunks"""
        
        import time
        start_time = time.time()
        
        print(f"\nğŸ“‹ Stage 6: Exact Metadata Search")
        
        query_variants = []
        all_results = []
        
        # 1. Extract structure references from query
        structure_refs = self.extract_structure_references(query)
        print(f"   Extracted structure refs: {structure_refs}")
        
        if structure_refs:
            # Build metadata filter for exact matching
            metadata_filters = {}
            
            for ref_type, ref_value in structure_refs:
                if ref_type == "Ä‘iá»u":
                    metadata_filters['article'] = f"Äiá»u {ref_value}"
                    print(f"   Filtering by article: Äiá»u {ref_value}")
                    
                elif ref_type == "khoáº£n":
                    metadata_filters['clause'] = ref_value
                    print(f"   Filtering by clause: {ref_value}")
                    
                elif ref_type == "Ä‘iá»ƒm":
                    metadata_filters['point'] = ref_value
                    print(f"   Filtering by point: {ref_value}")
                    
                elif ref_type == "chÆ°Æ¡ng":
                    metadata_filters['chapter'] = f"ChÆ°Æ¡ng {ref_value.upper()}"
                    print(f"   Filtering by chapter: ChÆ°Æ¡ng {ref_value.upper()}")
                    
                elif ref_type == "pháº§n":
                    metadata_filters['part'] = f"Pháº§n {ref_value}"
                    print(f"   Filtering by part: Pháº§n {ref_value}")
            
            # Find chunks that match ALL metadata filters
            if metadata_filters:
                matching_chunks = self.find_chunks_by_metadata(metadata_filters)
                print(f"   Found {len(matching_chunks)} exact matches")
                
                for chunk_id in matching_chunks:
                    if chunk_id in self.vector_store.vectors:
                        # Get the chunk data
                        metadata = self.vector_store.metadata.get(chunk_id, {})
                        content = metadata.get('content', '')
                        
                        # Create SearchResult with very high score for exact matches
                        from simple_vector_store import SearchResult
                        result = SearchResult(
                            chunk_id=chunk_id,
                            content=content,
                            score=1.0,  # Maximum score for exact metadata match
                            metadata=metadata
                        )
                        all_results.append(result)
                        query_variants.append(f"exact_match:{chunk_id}")
        
        # 2. If no exact matches, fall back to semantic search
        if not all_results:
            print(f"   No exact matches found, falling back to semantic search")
            
            # Extract hierarchy terms for semantic search
            hierarchy_terms = self.extract_hierarchy_terms(query)
            print(f"   Extracted hierarchy terms: {hierarchy_terms}")
            
            for term in hierarchy_terms:
                hierarchy_query = f"{term} {query}"
                query_variants.append(hierarchy_query)
                results = self.vector_store.search(query=hierarchy_query, top_k=6, min_score=0.3)
                all_results.extend(results)
        
        execution_time = time.time() - start_time
        
        print(f"   Query variants: {len(query_variants)}")
        print(f"   Results found: {len(all_results)}")
        print(f"   Execution time: {execution_time:.3f}s")
        
        return RetrievalStageResult(
            stage_name="Exact Metadata Search",
            results=all_results,
            query_variants=query_variants,
            execution_time=execution_time
        )
    
    def find_chunks_by_metadata(self, metadata_filters: Dict[str, str]) -> List[str]:
        """Find chunks that match ALL metadata filters exactly (case-insensitive for chapter)"""
        matching_chunks = []
        
        for chunk_id, metadata in self.vector_store.metadata.items():
            # Check if this chunk matches ALL filters
            matches_all = True
            
            for filter_key, filter_value in metadata_filters.items():
                chunk_value = metadata.get(filter_key, '')
                
                # Case-insensitive matching for chapter
                if filter_key == 'chapter':
                    if chunk_value.upper() != filter_value.upper():
                        matches_all = False
                        break
                else:
                    if chunk_value != filter_value:
                        matches_all = False
                        break
            
            if matches_all:
                matching_chunks.append(chunk_id)
        
        return matching_chunks
    
    def calculate_consensus_score(self, all_results: List[SearchResult]) -> Dict[str, Dict]:
        """
        Calculate consensus-based scoring dá»±a trÃªn:
        1. Stage weights
        2. Number of stages that found the result
        3. Combined confidence score
        """
        
        # Track results by chunk_id
        result_tracker = {}
        stage_names = ['stage1_semantic', 'stage3_keyword', 'stage5_crossref', 'stage6_exact']
        
        # Initialize tracking for each result
        for result in all_results:
            chunk_id = result.chunk_id
            
            if chunk_id not in result_tracker:
                result_tracker[chunk_id] = {
                    'chunk_id': chunk_id,
                    'content': result.content,
                    'metadata': result.metadata,
                    'stages_found': [],  # Which stages found this result
                    'stage_scores': {},  # Score from each stage
                    'max_stage_score': 0.0,
                    'consensus_count': 0,
                    'weighted_sum': 0.0
                }
        
        # Map stage results to stage names using index
        stage_names_list = []
        for stage_result in self.stage_results:
            if 'Universal Semantic' in stage_result.stage_name:
                stage_names_list.append('stage1_semantic')
            elif 'Universal Keywords' in stage_result.stage_name:
                stage_names_list.append('stage3_keyword')
            elif 'Cross-reference' in stage_result.stage_name:
                stage_names_list.append('stage5_crossref')
            elif 'Exact Metadata' in stage_result.stage_name:
                stage_names_list.append('stage6_exact')
            else:
                stage_names_list.append('unknown')
        
        # Count appearances in each stage
        for i, stage_result in enumerate(self.stage_results):
            stage_name = stage_names_list[i] if i < len(stage_names_list) else 'unknown'
            
            for result in stage_result.results:
                chunk_id = result.chunk_id
                
                if chunk_id in result_tracker:
                    # Add to stages found
                    if stage_name not in result_tracker[chunk_id]['stages_found']:
                        result_tracker[chunk_id]['stages_found'].append(stage_name)
                        result_tracker[chunk_id]['consensus_count'] += 1
                    
                    # Store stage score
                    result_tracker[chunk_id]['stage_scores'][stage_name] = result.score
                    result_tracker[chunk_id]['max_stage_score'] = max(
                        result_tracker[chunk_id]['max_stage_score'], 
                        result.score
                    )
                    
                    # Calculate weighted sum
                    stage_weight = self.stage_weights.get(stage_name, 1.0)
                    result_tracker[chunk_id]['weighted_sum'] += result.score * stage_weight
        
        return result_tracker
    
    def consensus_rerank(self, all_results: List[SearchResult]) -> List[SearchResult]:
        """
        Rerank results based on consensus scoring:
        1. Number of stages that found the result (consensus)
        2. Stage weights
        3. Combined confidence score
        """
        
        # Calculate consensus scores
        consensus_data = self.calculate_consensus_score(all_results)
        
        # Create new SearchResult objects with consensus scores
        consensus_results = []
        
        for chunk_id, data in consensus_data.items():
            # Calculate consensus score
            consensus_count = data['consensus_count']
            weighted_sum = data['weighted_sum']
            max_stage_score = data['max_stage_score']
            
            # Consensus factor: more stages = higher confidence
            consensus_factor = consensus_count / len(self.stage_results)  # 0.25, 0.5, 0.75, 1.0
            
            # Weight factor: sum of weighted scores
            weight_factor = weighted_sum / sum(self.stage_weights.values())  # Normalize
            
            # Final consensus score
            consensus_score = (consensus_factor * 0.4) + (weight_factor * 0.4) + (max_stage_score * 0.2)
            
            # Special bonus for exact matches (Stage 6)
            if 'stage6_exact' in data['stages_found']:
                consensus_score = max(consensus_score, 0.95)  # Minimum score for exact matches
            
            # Create new SearchResult with consensus score
            from simple_vector_store import SearchResult
            consensus_result = SearchResult(
                chunk_id=chunk_id,
                content=data['content'],
                score=consensus_score,
                metadata=data['metadata']
            )
            
            consensus_results.append(consensus_result)
        
        # Sort by consensus score
        consensus_results.sort(key=lambda x: x.score, reverse=True)
        
        return consensus_results
    
    def enhance_query(self, query: str) -> Tuple[str, Dict]:
        """
        Enhance query using LLM Query Enhancer
        Returns enhanced query and enhancement metadata
        """
        if not self.enable_query_enhancement or not self.query_enhancer:
            return query, {}
        
        try:
            enhancement = self.query_enhancer.enhance_query(query)
            
            # Return enhanced query and metadata
            enhancement_metadata = {
                'original_query': enhancement.original_query,
                'enhanced_query': enhancement.enhanced_query,
                'extracted_keywords': enhancement.extracted_keywords,
                'legal_terms': enhancement.legal_terms,
                'confidence_score': enhancement.confidence_score,
                'enhancement_type': enhancement.enhancement_type.value,
                'reasoning': enhancement.reasoning
            }
            
            return enhancement.enhanced_query, enhancement_metadata
            
        except Exception as e:
            print(f"âš ï¸ Query enhancement failed: {e}")
            return query, {}
    
    def extract_structure_references(self, query: str) -> List[Tuple[str, str]]:
        """Extract structure references from query (universal patterns)"""
        structure_refs = []
        
        # Universal patterns for legal documents (improved)
        patterns = [
            (r'[Ä‘d]iá»u\s+(\d+)', 'Ä‘iá»u'),        # "Äiá»u 180" hoáº·c "iá»u 180" â†’ ("Ä‘iá»u", "180")
            (r'khoáº£n\s+(\d+)', 'khoáº£n'),         # "Khoáº£n 2" â†’ ("khoáº£n", "2")
            (r'Ä‘iá»ƒm\s+([a-z])', 'Ä‘iá»ƒm'),         # "Äiá»ƒm a" â†’ ("Ä‘iá»ƒm", "a")
            (r'(\d+)\.\s*[a-z]', 'khoáº£n'),       # "1. a)" â†’ ("khoáº£n", "1")
            (r'chÆ°Æ¡ng\s+([ivx]+)', 'chÆ°Æ¡ng'),    # "ChÆ°Æ¡ng I" â†’ ("chÆ°Æ¡ng", "I")
            (r'pháº§n\s+([ivx]+)', 'pháº§n'),        # "Pháº§n I" â†’ ("pháº§n", "I")
            (r'má»¥c\s+(\d+)', 'má»¥c'),             # "Má»¥c 1" â†’ ("má»¥c", "1")
        ]
        
        for pattern, ref_type in patterns:
            matches = re.findall(pattern, query.lower())
            for match in matches:
                structure_refs.append((ref_type, match))
        
        return structure_refs
    
    def detect_content_patterns(self, query: str) -> List[str]:
        """Detect content patterns in query (universal approach)"""
        content_patterns = []
        
        # Universal content indicators
        content_indicators = [
            "quy Ä‘á»‹nh", "Ä‘iá»u kiá»‡n", "trÆ°á»ng há»£p", "ngoáº¡i lá»‡", "miá»…n",
            "thá»§ tá»¥c", "quy trÃ¬nh", "cÃ¡ch thá»©c", "xá»­ lÃ½", "Ã¡p dá»¥ng",
            "hiá»‡u lá»±c", "thi hÃ nh", "thá»±c hiá»‡n", "tuÃ¢n thá»§", "pháº¡t tiá»n",
            "pháº¡t tÃ¹", "hÃ¬nh pháº¡t"
        ]
        
        for indicator in content_indicators:
            if indicator in query.lower():
                content_patterns.append(indicator)
        
        return content_patterns
    
    def extract_hierarchy_terms(self, query: str) -> List[str]:
        """Extract hierarchy terms from query (universal)"""
        hierarchy_terms = []
        
        # Universal hierarchy terms
        hierarchy_indicators = ["pháº§n", "chÆ°Æ¡ng", "Ä‘iá»u", "khoáº£n", "Ä‘iá»ƒm", "má»¥c"]
        
        for term in hierarchy_indicators:
            if term in query.lower():
                hierarchy_terms.append(term)
        
        return hierarchy_terms
    
    def find_cross_references(self, query: str) -> List[str]:
        """TÃ¬m cross-references má»™t cÃ¡ch dynamic"""
        
        cross_refs = []
        
        # Extract article/chapter references from query
        article_matches = re.findall(r'Ä‘iá»u\s*(\d+)', query.lower())
        chapter_matches = re.findall(r'chÆ°Æ¡ng\s*([ivx]+)', query.lower())
        
        for article_num in article_matches:
            cross_refs.append(f"Ä‘iá»u {article_num}")
        
        for chapter_num in chapter_matches:
            cross_refs.append(f"chÆ°Æ¡ng {chapter_num}")
        
        return list(set(cross_refs))  # Remove duplicates
    
    def extract_hierarchies_from_results(self, results: List[SearchResult]) -> List[Dict[str, str]]:
        """Extract hierarchies tá»« previous results"""
        
        hierarchies = []
        
        for result in results[:3]:  # Top 3 results
            metadata = result.metadata
            
            if metadata.get('chapter'):
                hierarchies.append({'chapter': metadata['chapter']})
            
            if metadata.get('part'):
                hierarchies.append({'part': metadata['part']})
        
        # Remove duplicates
        unique_hierarchies = []
        for h in hierarchies:
            if h not in unique_hierarchies:
                unique_hierarchies.append(h)
        
        return unique_hierarchies
    
    def is_semantically_relevant(self, query: str, content: str, threshold: float = 0.5) -> bool:
        """Check semantic relevance giá»¯a query vÃ  content"""
        
        # Simple approach: check keyword overlap
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())
        
        overlap = len(query_words.intersection(content_words))
        relevance_score = overlap / len(query_words) if query_words else 0
        
        return relevance_score >= threshold
    
    def deduplicate_results(self, all_results: List[SearchResult]) -> List[SearchResult]:
        """Remove duplicate results"""
        
        seen_chunks: Set[str] = set()
        unique_results = []
        
        for result in all_results:
            if result.chunk_id not in seen_chunks:
                seen_chunks.add(result.chunk_id)
                unique_results.append(result)
        
        return unique_results
    
    def multi_factor_rerank(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """Multi-factor reranking with exact match priority"""
        
        def calculate_comprehensive_score(result: SearchResult) -> float:
            # Priority 1: Exact metadata matches (score = 1.0)
            if result.score == 1.0:
                return 1.0
            
            score = 0.0
            
            # Factor 1: Original similarity (30%)
            score += result.score * 0.3
            
            # Factor 2: Content quality (25%)
            content_length = len(result.content)
            content_bonus = min(content_length / 500, 1.0) * 0.25
            score += content_bonus
            
            # Factor 3: Legal hierarchy importance (25%)
            hierarchy_bonus = 0.0
            if result.metadata.get('type') == 'article':
                hierarchy_bonus = 0.25
            elif result.metadata.get('chapter'):
                hierarchy_bonus = 0.15
            score += hierarchy_bonus
            
            # Factor 4: Keyword density (20%)
            query_words = set(query.lower().split())
            content_words = set(result.content.lower().split())
            
            if query_words:
                keyword_density = len(query_words.intersection(content_words)) / len(query_words)
                score += keyword_density * 0.2
            
            return min(score, 0.99)  # Cap at 0.99 for non-exact matches
        
        # Sort by comprehensive score
        reranked = sorted(results, key=calculate_comprehensive_score, reverse=True)
        
        return reranked
    
    def execute_multi_stage_retrieval(self, query: str, max_results: int = 5) -> Tuple[List[SearchResult], List[RetrievalStageResult]]:
        """Execute complete multi-stage retrieval pipeline - 4 stages only"""
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ UNIVERSAL MULTI-STAGE RETRIEVAL (4 Stages)")
        print(f"Original Query: '{query}'")
        print(f"{'='*60}")
        
        # Enhance query using LLM if enabled
        enhanced_query = query
        enhancement_metadata = {}
        if self.enable_query_enhancement:
            print(f"\nğŸ”§ QUERY ENHANCEMENT:")
            enhanced_query, enhancement_metadata = self.enhance_query(query)
            
            if enhancement_metadata:
                print(f"   Original: {enhancement_metadata.get('original_query', query)}")
                print(f"   Enhanced: {enhancement_metadata.get('enhanced_query', query)}")
                print(f"   Keywords: {enhancement_metadata.get('extracted_keywords', [])}")
                print(f"   Legal Terms: {enhancement_metadata.get('legal_terms', [])}")
                print(f"   Confidence: {enhancement_metadata.get('confidence_score', 0):.2f}")
                print(f"   Type: {enhancement_metadata.get('enhancement_type', 'none')}")
        
        # Use enhanced query for retrieval
        retrieval_query = enhanced_query
        
        self.stage_results = []
        all_results = []
        
        # Execute 4 stages only (removed hardcoded Stage 2 and Stage 4)
        stage1_result = self.stage1_universal_semantic(retrieval_query)
        self.stage_results.append(stage1_result)
        all_results.extend(stage1_result.results)
        
        stage3_result = self.stage3_keyword_extraction(retrieval_query)
        self.stage_results.append(stage3_result)
        all_results.extend(stage3_result.results)
        
        # Store results for cross-reference stage
        self.previous_results = all_results
        
        stage5_result = self.stage5_cross_reference_search(retrieval_query)
        self.stage_results.append(stage5_result)
        all_results.extend(stage5_result.results)
        
        # Exact metadata search (new Stage 6)
        stage6_result = self.stage6_exact_metadata_search(retrieval_query)
        self.stage_results.append(stage6_result)
        all_results.extend(stage6_result.results)
        
        # Post-processing
        print(f"\nğŸ”„ Post-processing...")
        print(f"   Total raw results: {len(all_results)}")
        
        # Deduplicate
        unique_results = self.deduplicate_results(all_results)
        print(f"   After deduplication: {len(unique_results)}")
        
        # Consensus-based rerank (NEW)
        final_results = self.consensus_rerank(unique_results)
        print(f"   After consensus reranking: {len(final_results)}")
        
        # Take top results
        top_results = final_results[:max_results]
        
        print(f"\nâœ… FINAL RESULTS (Top {len(top_results)}):")
        consensus_data = self.calculate_consensus_score(unique_results)
        
        for i, result in enumerate(top_results, 1):
            article = result.metadata.get('article', 'N/A') if result.metadata else 'N/A'
            title = result.metadata.get('title', 'N/A') if result.metadata else 'N/A'
            # Safe string slicing - handle None values
            title_safe = str(title) if title is not None else 'N/A'
            title_short = title_safe[:50] if len(title_safe) > 50 else title_safe
            
            # Show consensus information
            chunk_data = consensus_data.get(result.chunk_id, {})
            consensus_count = chunk_data.get('consensus_count', 0)
            stages_found = chunk_data.get('stages_found', [])
            
            print(f"   {i}. {article} - {title_short}... (score: {result.score:.3f})")
            print(f"      ğŸ¯ Consensus: {consensus_count}/4 stages | Stages: {', '.join(stages_found)}")
        
        # Execution summary
        total_time = sum(stage.execution_time for stage in self.stage_results)
        print(f"\nğŸ“Š Execution Summary:")
        print(f"   Total execution time: {total_time:.3f}s")
        for stage in self.stage_results:
            print(f"   â€¢ {stage.stage_name}: {stage.execution_time:.3f}s ({len(stage.results)} results)")
        
        return top_results, self.stage_results


def main():
    """Test Universal Multi-stage Retrieval vá»›i cÃ¡c chá»§ Ä‘á» khÃ¡c nhau"""
    
    # Load vector store
    from simple_vector_store import SimpleVectorStore
    import os
    
    vector_store = SimpleVectorStore()
    if not vector_store.load('legal_vectors.pkl'):
        print("âŒ KhÃ´ng thá»ƒ load vector store")
        return
    
    # Initialize Universal Multi-stage Retrieval
    umsr = UniversalMultiStageRetrieval(vector_store)
    
    # Test vá»›i cÃ¡c queries phÃ¹ há»£p vá»›i clause-level chunks
    test_queries = [
        "Äiá»u 180 pháº¡t tiá»n bao nhiÃªu?",  # Article-specific
        "Khoáº£n 2 Äiá»u 225 quy Ä‘á»‹nh gÃ¬?",  # Clause-specific
        "Äiá»u 3 khoáº£n 1 vá» trÃ¡ch nhiá»‡m hÃ¬nh sá»±",  # Article + clause
        "pháº¡t tiá»n tá»« 50 triá»‡u Ä‘áº¿n 300 triá»‡u",  # Content-specific
        "ngÆ°á»i dÆ°á»›i 16 tuá»•i cÃ³ bá»‹ truy cá»©u trÃ¡ch nhiá»‡m hÃ¬nh sá»± khÃ´ng"  # General legal question
    ]
    
    for query in test_queries:
        print(f"\n" + "="*80)
        print(f"ğŸ§ª TESTING QUERY: '{query}'")
        print(f"="*80)
        
        try:
            results, stage_results = umsr.execute_multi_stage_retrieval(query, max_results=3)
            
            print(f"\nğŸ“‹ DETAILED RESULTS:")
            for i, result in enumerate(results, 1):
                print(f"\nResult {i}:")
                print(f"  Article: {result.metadata.get('article', 'N/A')}")
                print(f"  Title: {result.metadata.get('title', 'N/A')}")
                print(f"  Score: {result.score:.3f}")
                print(f"  Content preview: {result.content[:150]}...")
            
        except Exception as e:
            print(f"âŒ Error processing query: {e}")
        
        input("\nPress Enter to continue to next query...")


if __name__ == "__main__":
    main()
