import torch
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
from pyvi import ViTokenizer
import numpy as np

class CombinedScorer:
    def __init__(self, embedding_model_id="huyydangg/DEk21_hcmute_embedding", 
                 rerank_model_id="hghaan/rerank_model"):
        """Khá»Ÿi táº¡o scorer vá»›i embedding vÃ  reranker models"""
        self.embedding_model = SentenceTransformer(embedding_model_id)
        
        self.rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_id, trust_remote_code=True)
        self.rerank_model = AutoModel.from_pretrained(rerank_model_id, trust_remote_code=True, device_map="auto")
        
        # Trá»ng sá»‘ máº·c Ä‘á»‹nh (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)
        self.embedding_weight = 0.4  # 40% cho embedding
        self.reranker_weight = 0.6   # 60% cho reranker
        
    def normalize_embedding_score(self, sim_score):
        """Chuyá»ƒn cosine similarity (-1,1) thÃ nh (0,1)"""
        return (sim_score + 1) / 2
    
    def normalize_reranker_score(self, rerank_score, min_score=8, max_score=15):
        """Chuáº©n hÃ³a reranker score thÃ nh (0,1) dá»±a trÃªn min/max observed"""
        # Clip score trong khoáº£ng min/max
        clipped_score = np.clip(rerank_score, min_score, max_score)
        # Normalize vá» (0,1)
        return (clipped_score - min_score) / (max_score - min_score)
    
    def get_embedding_score(self, query, documents):
        """TÃ­nh embedding score cho documents"""
        # Tokenize Vietnamese
        segmented_query = ViTokenizer.tokenize(query)
        segmented_docs = [ViTokenizer.tokenize(doc) for doc in documents]
        
        # Encode
        query_emb = self.embedding_model.encode([segmented_query])
        doc_embs = self.embedding_model.encode(segmented_docs)
        
        # Cosine similarity
        similarities = F.cosine_similarity(torch.tensor(query_emb), torch.tensor(doc_embs)).flatten()
        
        # Normalize to (0,1)
        normalized_scores = [self.normalize_embedding_score(sim.item()) for sim in similarities]
        
        return normalized_scores
    
    def get_reranker_score(self, query, documents):
        """TÃ­nh reranker score cho documents"""
        scores = []
        
        for doc in documents:
            inputs = self.rerank_tokenizer(
                f"query: {query} document: {doc}", 
                return_tensors="pt", 
                truncation=True, 
                padding=True
            ).to(self.rerank_model.device)
            
            with torch.no_grad():
                outputs = self.rerank_model(**inputs)
            
            # Sá»­ dá»¥ng CLS norm nhÆ° trÆ°á»›c
            if hasattr(outputs, 'last_hidden_state'):
                cls_embedding = outputs.last_hidden_state[:, 0, :]
                score = torch.norm(cls_embedding, dim=-1).item()
            else:
                score = outputs.last_hidden_state.mean().item()
            
            scores.append(score)
        
        # Normalize scores
        normalized_scores = [self.normalize_reranker_score(score) for score in scores]
        
        return normalized_scores
    
    def get_combined_score(self, query, documents, embedding_weight=None, reranker_weight=None):
        """TÃ­nh Ä‘iá»ƒm tá»•ng há»£p vá»›i trá»ng sá»‘"""
        if embedding_weight is None:
            embedding_weight = self.embedding_weight
        if reranker_weight is None:
            reranker_weight = self.reranker_weight
        
        # TÃ­nh cÃ¡c score riÃªng láº»
        emb_scores = self.get_embedding_score(query, documents)
        rerank_scores = self.get_reranker_score(query, documents)
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng há»£p
        combined_scores = []
        for emb, rerank in zip(emb_scores, rerank_scores):
            combined = embedding_weight * emb + reranker_weight * rerank
            combined_scores.append(combined)
        
        return combined_scores, emb_scores, rerank_scores
    
    def get_confidence_percentage(self, combined_score):
        """Chuyá»ƒn Ä‘iá»ƒm tá»•ng há»£p thÃ nh pháº§n trÄƒm tin cáº­y"""
        # Sá»­ dá»¥ng sigmoid Ä‘á»ƒ chuyá»ƒn (0,1) thÃ nh pháº§n trÄƒm
        confidence = 1 / (1 + np.exp(-10 * (combined_score - 0.5))) * 100
        return min(100, max(0, confidence))  # Clip trong [0, 100]

def main():
    # Khá»Ÿi táº¡o scorer
    scorer = CombinedScorer()
    
    # Test data
    query = "NgÆ°á»i lao Ä‘á»™ng cÃ³ quyá»n nghá»‰ thai sáº£n bao lÃ¢u?"
    documents = [
        "Theo Ä‘iá»u 139 Bá»™ luáº­t Lao Ä‘á»™ng 2019, lao Ä‘á»™ng ná»¯ Ä‘Æ°á»£c nghá»‰ trÆ°á»›c vÃ  sau khi sinh con lÃ  6 thÃ¡ng.",
        "NgÆ°á»i lao Ä‘á»™ng cÃ³ quyá»n nghá»‰ phÃ©p háº±ng nÄƒm theo thá»a thuáº­n vá»›i ngÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng.",
        "Thá»i tiáº¿t hÃ´m nay ráº¥t Ä‘áº¹p, nhiá»u ngÆ°á»i Ä‘i chÆ¡i.",
        "Trong lÄ©nh vá»±c AI, machine learning lÃ  má»™t nhÃ¡nh quan trá»ng.",
        "CÃ´ng nghá»‡ blockchain ngÃ y cÃ ng phÃ¡t triá»ƒn máº¡nh máº½."
    ]
    
    print("=" * 80)
    print("Há»† THá»NG TÃNH ÄIá»‚M Tá»”NG Há»¢P VÃ€ Äá»˜ TIN Cáº¬Y")
    print("=" * 80)
    
    # Test vá»›i trá»ng sá»‘ máº·c Ä‘á»‹nh
    print(f"\nğŸ“Š Trá»ng sá»‘: Embedding {scorer.embedding_weight*100:.0f}% | Reranker {scorer.reranker_weight*100:.0f}%")
    combined, emb, rerank = scorer.get_combined_score(query, documents)
    
    # Sáº¯p xáº¿p theo Ä‘iá»ƒm tá»•ng há»£p
    results = list(zip(documents, combined, emb, rerank))
    results.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ¯ Query: {query}")
    print("\nğŸ“ˆ Káº¾T QUáº¢ Xáº¾P Háº NG:")
    print("-" * 80)
    
    for i, (doc, comb, emb_score, rerank_score) in enumerate(results, 1):
        confidence = scorer.get_confidence_percentage(comb)
        print(f"{i}. Äá»™ tin cáº­y: {confidence:.1f}%")
        print(f"   ğŸ“ Document: {doc[:80]}...")
        print(f"   ğŸ”¢ Chi tiáº¿t: Combined={comb:.3f} | Embedding={emb_score:.3f} | Reranker={rerank_score:.3f}")
        print()
    
    # Test vá»›i trá»ng sá»‘ khÃ¡c
    print("\n" + "=" * 80)
    print("ğŸ”„ THá»¬ NGHIá»†M Vá»šI TRá»ŒNG Sá» KHÃC")
    print("=" * 80)
    
    weight_configs = [
        (0.5, 0.5, "CÃ¢n báº±ng"),
        (0.7, 0.3, "Æ¯u tiÃªn Embedding"),
        (0.2, 0.8, "Æ¯u tiÃªn Reranker")
    ]
    
    for emb_w, rerank_w, desc in weight_configs:
        print(f"\nâš–ï¸ {desc} (Embedding: {emb_w*100:.0f}%, Reranker: {rerank_w*100:.0f}%)")
        combined, _, _ = scorer.get_combined_score(query, documents, emb_w, rerank_w)
        
        # Top 2 results
        results = list(zip(documents, combined))
        results.sort(key=lambda x: x[1], reverse=True)
        
        for i, (doc, comb) in enumerate(results[:2], 1):
            confidence = scorer.get_confidence_percentage(comb)
            print(f"  {i}. {confidence:.1f}% - {doc[:60]}...")

if __name__ == "__main__":
    main()
